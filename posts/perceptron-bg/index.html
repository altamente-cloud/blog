<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><title>Перцептронът - основа на невронните мрежи - learn.altamente.cloud</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=/favicon.png><link rel=canonical href=/posts/perceptron-bg/><link rel=stylesheet href=/css/style.min.9ace1889e342b746efbfbd99b2bcd2af4d96d96fa6c8dac480411cf33a84c4fd.css><link rel=stylesheet href=/assets/css/extended.min.75f2a96901bf25eddcf37edc9a30bebc25c59bb8d45432f08aef5c56e7a38d1f.css><meta name=description content="Python, Conda, Jupyter - инструменти за работа с ML"><meta property="og:title" content="Перцептронът - основа на невронните мрежи"><meta property="og:type" content="website"><meta property="og:url" content="/posts/perceptron-bg/"><meta property="og:image" content="/images/perceptron/perceptron_header.png"><meta property="og:description" content="Python, Conda, Jupyter - инструменти за работа с ML"><meta name=twitter:card content="summary"><meta name=twitter:site content="@zerostaticio"><meta name=twitter:creator content="@zerostaticio"><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&amp;display=swap" rel=stylesheet></head><body class='page frame page-blog-single'><div id=menu-main-mobile class=menu-main-mobile><ul class=menu><li class=menu-item-начало><a href=/>Начало</a></li><li class=menu-item-статии><a href=/posts/>Статии</a></li><li class="menu-item-за сайта"><a href=/about/>За сайта</a></li><li class=menu-item-по теми><a href=/tags/>По теми</a></li></ul></div><div id=wrapper class=wrapper><div class=header><a class=header-logo href=/>learn.altamente.cloud</a><div class=menu-main><ul><li class=menu-item-начало><a href=/><span>Начало</span></a></li><li class=menu-item-статии><a href=/posts/><span>Статии</span></a></li><li class="menu-item-за сайта"><a href=/about/><span>За сайта</span></a></li><li class=menu-item-по теми><a href=/tags/><span>По теми</span></a></li></ul></div><div id=toggle-menu-main-mobile class=hamburger-trigger><button class=hamburger>Menu</button></div></div><div class=blog><div class=intro><h1>Перцептронът - основа на невронните мрежи<span class=dot>.</span></h1><img alt=Perceptron src=/images/perceptron/perceptron_header.png><p class=image-description>Изображение генерирано от ChatGPT</p></div><div class=tags><a href=/%20/tags/perceptron class=tag>perceptron</a>
<a href=/%20/tags/math class=tag>math</a></div><div class=content><h2 id=въведение>Въведение</h2><p>Работата на Франк Розенблат (Frank Rosenblatt) постави основите на съвременните невронни мрежи. Той създаде принципи, които остават актуални и до днес, и положи фундамента за по-нататъшни изследвания в областта на машинното обучение. В тази статия ще представя основните идеи, които той предложи, и ще опитам да проведа паралели със съвременните невронни мрежи. Също така ще покажа как да реализираме перцептрон на Python с използването на библиотеката NumPy.</p><h2 id=какво-е-неврон>Какво е неврон?</h2><p>Нашият мозък се състои от милиарди клетки, наречени неврони. Невронът е основната единица на нервната система, която отговаря за предаването и обработката на информацията.</p><p>Неврон или нервна клетка (от др.-гр. νεῦρον &ldquo;влакно; нерв&rdquo;) — тясно специализирана клетка. Невронът е електрически възбудима клетка, предназначена за приемане извън, обработване, съхранение, предаване и извеждане извън информацията с помощта на електрически и химически сигнали. <a href=https://bg.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B2%D1%80%D0%BE%D0%BD>(Wikipedia)</a></p><p>Всеки неврон получава сигнали от други неврони и ги предава по-нататък. Невроните са свързани помежду си чрез синапси, които им позволяват да обменят информация.</p><figure><img src=/images/perceptron/neuron.png alt=Neuron><figcaption>Схема на неврон (by BruceBlaus взето от Wikipedia)</figcaption></figure><p>Да обърнем внимание на дендритите и аксона. Дендритите приемат сигнали от други неврони, а аксонът предава сигнали по-нататък. Всеки неврон може да има множество дендрити и един аксон (в повечето случаи). Сигналите, които невронът получава от други неврони, могат да бъдат както възбуждащи, така и потискащи. Възбуждащите сигнали увеличават вероятността невронът да &ldquo;сработи&rdquo; (т.е. да предаде сигнал нататък), а потискащите — намаляват тази вероятност.</p><p>Така през 1957 г. Франк Розенблат предложи модел на неврон, който нарече перцептрон.</p><h2 id=какво-е-перцептрон>Какво е перцептрон?</h2><p>Перцептронът е най-простият модел на невронна мрежа, която се състои от един неврон. Той приема няколко входни сигнала, обработва ги и дава един изходен сигнал.</p><figure><img src=/images/perceptron/perceptron.png alt=Perceptron><figcaption>Схема на перцептрон</figcaption></figure><p>В своя модел, Розенблат използва няколко входа, всеки от които има свое тегло. Теглото е число, което показва колко важен е този вход за неврона. Колкото по-голямо е теглото, толкова по-голямо е влиянието на този вход върху изхода на неврона.<br>Входовете и теглата се умножават, и след това всички произведения се сумират. Ако сумата е по-голяма от прага, то невронът &ldquo;сработва&rdquo; и дава сигнал 1, иначе — 0. Прагът е число, което показва колко силен трябва да бъде сигналът, за да &ldquo;сработи&rdquo; невронът. Прагът може да се счита за тегло, което се умножава по 1 (входът винаги е равен на 1).</p><p>В съвременните невронни мрежи прагът обикновено се заменя с функция на активация, която позволява на неврона да &ldquo;сработва&rdquo; при определени условия. Функцията на активация може да бъде линейна или нелинейна. При линейна функция на активация изходът на неврона е пропорционален на входа, а при нелинейна — изходът зависи от входа по сложна формула.</p><p>Нека запишем формулата на перцептрона:</p><p>Ако на входа имаме вектор \(x = (x_1, x_2, \ldots, x_n)\), а на изхода — число \(y\). Тогава формулата на перцептрона изглежда така:</p><p>Сума на всички входове, умножени по теглата:<br></p>$$ z^{(i)} = \theta_0+\theta_1x_1^{(i)} + \theta_2x_2^{(i)} + ... + \theta_nx_n^{(i)} = \theta^Tx^{(i)} \tag1$$<p><br>където i - номер на примера, \(\theta\) - вектор на теглата, \(x^{(i)}\) - вектор на входовете.</p><p>Тогава функцията на активация (стъпкова функция):</p>$$
h(z)=
\left\{
\begin{array}{rcr}
1 & z \geq 0 \\
0 & в\: противен\: случай \\
\end{array}
\right.
\tag2
$$<p>Нека напишем код, който реализира перцептрона. Ще използваме библиотеката NumPy за работа с масиви и матрици. Също така ще използваме библиотеката Matplotlib за визуализация на данните.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Perceptron</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>pass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>net_input</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>  <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>theta</span><span class=p>[</span><span class=mi>1</span><span class=p>:])</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>theta</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=c1># z</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>  <span class=n>np</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>net_input</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>&gt;=</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=c1># h(z)</span>
</span></span></code></pre></div><p>Вероятно сте забелязали в кода, че сме добавили единица към теглата. Това се прави, за да се вземе предвид прагът. Можем да считаме, че имаме допълнителен вход, който винаги е равен на 1. Този вход се умножава по тегло, което е равно на прага. По този начин можем да считаме, че прагът е тегло, което се умножава по 1.<br>Също така, първоначалните тегла не са ни известни, затова ги инициализираме с нули. И пред нас стои задачата да намерим такива тегла, които ще позволят на перцептрона правилно да класифицира данните.</p><h2 id=как-да-обучим-перцептрона>Как да обучим перцептрона?</h2><p>Преди да обучим перцептрона, нека определим какво е обучение. Обучението е процес, при който намираме такива тегла, които ще позволят на перцептрона правилно да класифицира данните. Т.е. имаме входни данни x₁&mldr;xₙ и изходни данни y (0 или 1), и искаме да намерим такива тегла \(\theta\), които ще позволят на перцептрона правилно да класифицира данните.</p><p>За тази цел трябва да определим функция на загубата. Функцията на загубата е функция, която показва колко добре перцептронът класифицира данните. Колкото по-малка е стойността на функцията на загубата, толкова по-добре перцептронът класифицира данните (толкова по-малки са загубите - оттук и името).</p>$$ error^{(i)} = \frac12(h(z^{(i)}) - y^{(i)})^2 \tag3$$<p>Нека разберем защо в тази формула имаме квадрат. Ако бихме използвали просто разликата, то при обучение перцептронът би могъл да се &ldquo;обърква&rdquo; в знаците.</p><p>Тогава общата функция на загуба ще бъде равна на:</p>$$
L(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h(z^{(i)}) - y^{(i)})^2 \tag4
$$<p>Делим резултата на m, за да получим средната стойност на загубите. m е броят на примерите в обучаващата извадка.</p><p>Ако предсказваме 1, а в действителност е 0, то грешката ще бъде равна на (1-0) = 1, а ако е обратното, то грешката ще бъде равна на (0-1) = -1. Ако бихме използвали просто разлика, то в първия случай грешката би била положителна, а във втория — отрицателна. И при сумиране грешките биха се взаимно унищожавали. Ако вземем квадрат, то в двата случая грешката ще бъде положителна. По този начин можем да считаме, че грешката е разстоянието от предсказанието до действителната стойност. Колкото по-малко е това разстояние, толкова по-добре перцептронът класифицира данните.<br>Тъй като данните са фиксирани, можем да ги считаме за константи. Тогава функцията на загубата ще зависи само от теглата \(\theta\). Т.е. можем да считаме, че <strong>функцията на загубата е функция от теглата.</strong></p><p>Тъй като нашата функция на загубата е квадратична, то нейната производна ще бъде линейна и ще е равна на 0 в точката на минимума. За опростяване на изчислението на производната, умножаваме функцията на загубата по 1/2 (това няма да повлияе на минимума, тъй като 1/2 е просто константа). Тогава производната за пример i ще бъде равна на:</p>$$
\frac{\partial{\frac12(\theta_0+\theta_1x_1^{(i)} + \theta_2x_2^{(i)} + ... + \theta_nx_n^{(i)} - y^{(i)} )^2}}{\partial{\theta_j}} = (h(z^{(i)}) - y^{(i)})x_j^{(i)}
$$<p>и общата производна по правилото за сумата ще бъде равна на:</p>$$
\frac{\partial{L(\theta)}}{\partial{\theta_j}} = \frac{1}{m} \sum_{i=1}^{m} (h(z^{(i)}) - y^{(i)})x_j^{(i)} \tag5
$$<p>за j = 0, 1, &mldr;, n</p><p>За да решим задачата аналитично, би трябвало да приравним всяка частна производна на нула и да решим уравнението. Но това по правило е невъзможно да се направи. Ако имаме 100 примера по 100 признака във всеки (m = 100 и x₁&mldr;x₁₀₀), би трябвало да решим система от 100 уравнения със 100 неизвестни. Затова ще използваме числени методи за намиране на минимума на функцията на загубата. Един от най-простите и популярни методи е методът на <strong>градиентното спускане</strong>.</p><h2 id=градиентно-спускане>Градиентно спускане</h2><p>Градиентното спускане е итеративен метод, който позволява да се намери минимум на функцията.</p><p>Идеята на този метод е в това, че ако променим теглото \(\theta_j\) с малка стойност, то функцията на загубата (3) ще се промени приблизително с:</p>$$
\Delta{error} =(h(z^{(i)}) - y^{(i)})x_j^{(i)}\Delta{\theta_j}
$$<p>Така, за да намалим грешката, трябва да променим теглото \(\theta_j\) в посока, противоположна на производната (тъй като производната показва посоката, в която функцията расте). Т.е. трябва да намалим теглото, ако производната е положителна, и да увеличим теглото, ако производната е отрицателна. Можем да запишем това във вид на формула:</p>$$
\Delta{\theta_j} = \alpha = learningrate(стъпка\: на\: обучение)
$$$$
\theta_j = \theta_j - \alpha(h(z^{(i)}) - y^{(i)}) x_j^{(i)} \tag6
$$<p>Стъпката на обучение не трябва да бъде твърде голяма, иначе можем да &ldquo;прескочим&rdquo; минимума и да започнем да се движим в друга посока. Ако стъпката на обучение е твърде малка, то процесът на обучение ще бъде твърде дълъг. Затова трябва да я подбираме експериментално.</p><h2 id=реализация>Реализация</h2><p>Нека сами напишем функция, която ще обучава перцептрона. Но първо да се определим с данните.</p><p>Един от най-популярните набори от данни за обучение на перцептрон е наборът от данни за ириси (Iris dataset). Той се състои от 150 примера, всеки от които има 4 признака (дължина и ширина на чашелистчето и венчелистчето) и 3 класа (видове ириси). Ние ще използваме само два класа (Setosa и Versicolor), за да опростим задачата <a href=https://en.wikipedia.org/wiki/Iris_flower_data_set>(Wikipedia)</a>.</p><p>Нека инсталираме необходимите библиотеки и заредим данните:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install numpy matplotlib pandas
</span></span></code></pre></div><p>Сега да стартираме Jupyter Notebook и да заредим данните:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=s1>&#39;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>header</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>tail</span><span class=p>()</span>
</span></span></code></pre></div><p>Резултат:</p><figure><img src=/images/perceptron/iris.png alt="Iris dataset"><figcaption>Набор от данни за ириси</figcaption></figure><p>Нека визуализираме данните:</p><figure><img src=/images/perceptron/iris_plot.png alt="Iris dataset"><figcaption>Набор от данни за ириси</figcaption></figure><p>Можем да забележим, че данните са линейно разделими. Т.е. можем да прокараме права линия, която ще раздели данните на два класа. Това означава, че можем да използваме перцептрон за решаване на тази задача. Ако данните не бяха линейно разделими, то би трябвало да използваме по-сложни модели (например, многослойни невронни мрежи).</p><p>Нека обновим кода, така че да включва обучение на перцептрона (метод fit) и масив с грешки за проследяване на процеса на обучение:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Perceptron</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>n_iter</span><span class=o>=</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=n>alpha</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>n_iter</span> <span class=o>=</span> <span class=n>n_iter</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>wrong_classifications</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>theta</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=c1># тегла</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>n_iter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>wrong_classification</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>xi</span><span class=p>,</span> <span class=n>target</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>update</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=n>target</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>xi</span><span class=p>))</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>theta</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span> <span class=o>+=</span> <span class=n>update</span> <span class=o>*</span> <span class=n>xi</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>theta</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+=</span> <span class=n>update</span>
</span></span><span class=line><span class=cl>                <span class=n>wrong_classification</span> <span class=o>+=</span> <span class=nb>int</span><span class=p>(</span><span class=n>update</span> <span class=o>!=</span> <span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>wrong_classifications</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>wrong_classification</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>  <span class=bp>self</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>net_input</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>  <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>theta</span><span class=p>[</span><span class=mi>1</span><span class=p>:])</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>theta</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=c1># z</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>  <span class=n>np</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>net_input</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>&gt;=</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=c1># h(z)</span>
</span></span></code></pre></div><p>Сега нека извикаме метода fit и му предадем данните:</p><figure><img src=/images/perceptron/learn.png alt="Iris dataset"><figcaption>Брой грешки спрямо броя стъпки</figcaption></figure><p>Както виждате, след 6 стъпки перцептронът се научи да класифицира данните. Сега нека погледнем границата на вземане на решение. Можем да визуализираме границата на вземане на решение, като използваме библиотеката Matplotlib.</p><figure><img src=/images/perceptron/result.png alt="Decision boundary" width=600/><figcaption>Граница на вземане на решение</figcaption></figure><h2 id=заключение>Заключение</h2><p>В тази статия разгледахме перцептрон — най-простия модел на невронна мрежа. Разбрахме как работи и как да го обучим. Също така разгледахме метода на градиентното спускане, който позволява да се намери минимум на функцията на загубата. Надявам се, че тази статия ви беше полезна и ви помогна по-добре да разберете как работят невронните мрежи.</p></div></div><div class=footer><div class=footer-social><span class="social-icon social-icon-twitter"><a href=https://twitter.com/zerostaticio title=twitter target=_blank rel=noopener><img src=/images/social/twitter.svg width=24 height=24 alt=twitter>
</a></span><span class="social-icon social-icon-github"><a href=https://github.com/zerostaticthemes/hugo-winston-theme title=github target=_blank rel=noopener><img src=/images/social/github.svg width=24 height=24 alt=github>
</a></span><span class="social-icon social-icon-linkedin"><a href=https://www.linkedin.com title=linkedin target=_blank rel=noopener><img src=/images/social/linkedin.svg width=24 height=24 alt=linkedin></a></span></div></div></div><script type=text/javascript src=/js/bundle.min.5993fcb11c07dea925a3fbd58c03c7f1857197c35fccce3aa963a12c0b3c9960.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]],packages:{"[+]":["unicode"]}},loader:{load:["ui/safe","[tex]/unicode"]},svg:{scale:1,minScale:.5,mtextInheritFont:!0,merrorInheritFont:!0,mathmlSpacing:!1,skipAttributes:{},exFactor:.5,displayAlign:"center",displayIndent:"0",fontCache:"local"},options:{enableMenu:!0,renderActions:{addMenu:[0,"",""]}},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(()=>{document.querySelectorAll("mjx-container").forEach(e=>{e.hasAttribute("display")?e.classList.add("math-display"):e.classList.add("math-inline")})})}}</script><style>.MathJax{margin:0 .15em}.MathJax[jax=SVG] text{font-family:inherit}</style></body></html>