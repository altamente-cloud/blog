<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><title>GGML - стартираме модели на всички устройства - learn.altamente.cloud</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=/favicon.png><link rel=canonical href=/posts/ggml-bg/><link rel=stylesheet href=/css/style.min.9ace1889e342b746efbfbd99b2bcd2af4d96d96fa6c8dac480411cf33a84c4fd.css><link rel=stylesheet href=/assets/css/extended.min.75f2a96901bf25eddcf37edc9a30bebc25c59bb8d45432f08aef5c56e7a38d1f.css><meta name=description content="Разбор на първата версия на библиотеката GGML - изучаваме архитектурата, тензорите, паметта и основните примитиви"><meta property="og:title" content="GGML - стартираме модели на всички устройства"><meta property="og:type" content="website"><meta property="og:url" content="/posts/ggml-bg/"><meta property="og:description" content="Разбор на първата версия на библиотеката GGML - изучаваме архитектурата, тензорите, паметта и основните примитиви"><meta name=twitter:card content="summary"><meta name=twitter:site content="@zerostaticio"><meta name=twitter:creator content="@zerostaticio"><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&amp;display=swap" rel=stylesheet></head><body class='page frame page-blog-single'><div id=menu-main-mobile class=menu-main-mobile><ul class=menu><li class=menu-item-начало><a href=/>Начало</a></li><li class=menu-item-статии><a href=/posts/>Статии</a></li><li class="menu-item-за сайта"><a href=/about/>За сайта</a></li><li class=menu-item-по теми><a href=/tags/>По теми</a></li></ul></div><div id=wrapper class=wrapper><div class=header><a class=header-logo href=/>learn.altamente.cloud</a><div class=menu-main><ul><li class=menu-item-начало><a href=/><span>Начало</span></a></li><li class=menu-item-статии><a href=/posts/><span>Статии</span></a></li><li class="menu-item-за сайта"><a href=/about/><span>За сайта</span></a></li><li class=menu-item-по теми><a href=/tags/><span>По теми</span></a></li></ul></div><div id=toggle-menu-main-mobile class=hamburger-trigger><button class=hamburger>Menu</button></div></div><div class=blog><div class=intro><h1>GGML - стартираме модели на всички устройства<span class=dot>.</span></h1></div><div class=tags><a href=/%20/tags/ggml class=tag>GGML</a>
<a href=/%20/tags/llm class=tag>LLM</a>
<a href=/%20/tags/ml class=tag>ML</a>
<a href=/%20/tags/c class=tag>C</a>
<a href=/%20/tags/%D1%82%D0%B5%D0%BD%D0%B7%D0%BE%D1%80%D0%B8 class=tag>тензори</a>
<a href=/%20/tags/%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5 class=tag>машинно обучение</a></div><div class=content><h1 id=ggml---llm-на-всички-устройства>GGML - LLM на всички устройства</h1><h2 id=съдържание>Съдържание</h2><ul><li><a href=#%D0%BA%D0%B0%D0%BA%D0%B2%D0%BE-%D0%B5-ggml>Какво е GGML</a></li><li><a href=#%D0%B8%D0%BD%D1%81%D1%82%D0%B0%D0%BB%D0%B0%D1%86%D0%B8%D1%8F-%D0%B8-%D1%81%D1%82%D0%B0%D1%80%D1%82%D0%B8%D1%80%D0%B0%D0%BD%D0%B5>Инсталация и стартиране</a></li><li><a href=#%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%BD%D0%B8-%D0%BF%D1%80%D0%B8%D0%BC%D0%B8%D1%82%D0%B8%D0%B2%D0%B8>Основни примитиви</a><ul><li><a href=#%D0%BF%D0%B0%D0%BC%D0%B5%D1%82>Памет</a><ul><li><a href=#%D1%82%D0%B5%D0%BD%D0%B7%D0%BE%D1%80>Тензор</a></li><li><a href=#%D0%BA%D0%BE%D0%BD%D1%82%D0%B5%D0%BA%D1%81%D1%82>Контекст</a></li><li><a href=#%D0%BF%D0%BE%D0%B4%D1%80%D0%B0%D0%B2%D0%BD%D1%8F%D0%B2%D0%B0%D0%BD%D0%B5>Подравняване</a></li><li><a href=#%D1%82%D0%B8%D0%BF%D0%BE%D0%B2%D0%B5-%D0%B4%D0%B0%D0%BD%D0%BD%D0%B8>Типове данни</a></li><li><a href=#%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8>Оптимизации</a></li></ul></li><li><a href=#%D1%82%D0%B5%D0%BD%D0%B7%D0%BE%D1%80%D0%B8-%D0%B2-%D0%BF%D0%B0%D0%BC%D0%B5%D1%82%D1%82%D0%B0>Тензори в паметта</a><ul><li><a href=#%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0-%D0%BD%D0%B0-%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%82%D0%B0>Проверка на размерността</a></li><li><a href=#%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0-%D0%BD%D0%B0-%D1%81%D0%BC%D0%B5%D0%B6%D0%BD%D0%BE%D1%81%D1%82--padding>Проверка на смежност / padding</a></li><li><a href=#%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B0-%D0%BD%D0%B0-%D1%81%D0%BC%D0%B5%D0%B6%D0%BD%D0%BE%D1%81%D1%82-%D0%B1%D0%B5%D0%B7-%D0%BF%D1%8A%D1%80%D0%B2%D0%BE%D1%82%D0%BE-%D0%B8%D0%B7%D0%BC%D0%B5%D1%80%D0%B5%D0%BD%D0%B8%D0%B5>Проверка на смежност без първото измерение</a></li><li><a href=#%D1%80%D0%B0%D0%B7%D0%BC%D0%B5%D1%80-%D0%BD%D0%B0-%D1%82%D0%B5%D0%BD%D0%B7%D0%BE%D1%80%D0%B8%D1%82%D0%B5>Размер на тензорите</a></li></ul></li><li><a href=#%D1%81%D1%8A%D0%B7%D0%B4%D0%B0%D0%B2%D0%B0%D0%BD%D0%B5-%D0%BD%D0%B0-%D1%82%D0%B5%D0%BD%D0%B7%D0%BE%D1%80%D0%B8>Създаване на тензори</a></li></ul></li><li><a href=#%D0%B0%D0%B2%D1%82%D0%BE%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D0%BE-%D0%B4%D0%B8%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D1%80%D0%B0%D0%BD%D0%B5-%D0%B8-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5>Автоматично диференциране и обучение</a><ul><li><a href=#%D0%B2%D1%8A%D0%B7%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D0%B8-%D0%BD%D0%B0-ggml-%D0%B7%D0%B0-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5>Възможности на GGML за обучение</a></li><li><a href=#%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80-%D0%B7%D0%B0-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D0%BD%D0%B0-%D0%BB%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D0%B8%D1%8F>Пример за обучение на линейна регресия</a></li></ul></li></ul><h2 id=какво-е-ggml>Какво е GGML</h2><p>В момента PyTorch е най-популярният framework (заместил TensorFlow) за работа с ML. Основната задача на такива frameworks е работата с тензори (структури от данни, които представляват скалари, вектори или многомерни масиви). Библиотеките предоставят възможност за извършване на математически операции, намиране на градиенти (изчисляване на стойностите на производната на функцията в дадена точка), съдържат най-популярните алгоритми и позволяват използването на предимствата на устройствата, на които се използва библиотеката, например извършване на изчисления на GPU или използване на специални математически копроцесори или специфични CPU инструкции, които позволяват ускоряване на изчисленията на този тип устройства. Основният недостатък, когато се опитате да покриете всички сценарии, е размерът на framework-а и всички зависимости.</p><p>В противовес на всеобхватния подход, Георги Герганов от гр. София създаде библиотеката GGML (оттук и името: GG - инициалите на автора и ML). Идеята беше да се напише такава библиотека, която би позволила стартирането на LLM на голям брой устройства с МИНИМАЛНИ изисквания, като Raspberry Pi или слаби лаптопи. Тя позволява разделянето на изчисленията между CPU и GPU и компресирането (квантоването на моделите), като по този начин значително намалява размера на моделите, незначително губейки качество, но предоставяйки на потребителя възможността сам да решава какво да постави в приоритет: размер или качество.</p><p>В днешно време GGML поддържа изчисления на различни архитектури (NVIDIA, AMD, Apple Metal, ARM и т.н.). Но аз, вместо да се занимавам с всички детайли, искам да разгледам първата версия (първия commit от GitHub репозиторито) и да видя с какво всичко започна. В тази версия още нямаше поддръжка на GPU, но като основа за запознаване - това е отличен пример. Затова ако ви интересува как да използвате библиотеката днес, тази статия не е за вас.<br>Така че, нека видим с какво всичко започна и да се опитаме да разберем как работи всичко това и да стартираме проста модел. Хайде!</p><h2 id=инсталация-и-стартиране>Инсталация и стартиране</h2><p>Клонираме проекта и се превключваме към първия commit</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/ggml-org/ggml
</span></span><span class=line><span class=cl><span class=nb>cd</span> ggml
</span></span><span class=line><span class=cl>git checkout fb558f
</span></span></code></pre></div><p>Целият код, без заглавните файлове - в един файл!!! : src/ggml.c</p><p>Нека опитаме да компилираме проекта, за да проверим че всичко е на място.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>❯ <span class=nb>cd</span> ..
</span></span><span class=line><span class=cl>❯ mkdir build
</span></span><span class=line><span class=cl>❯ <span class=nb>cd</span> build
</span></span><span class=line><span class=cl>❯ cmake ../ggml/ -DCMAKE_POLICY_VERSION_MINIMUM<span class=o>=</span>3.5
</span></span><span class=line><span class=cl>-- The C compiler identification is GNU 15.1.1
</span></span><span class=line><span class=cl>-- The CXX compiler identification is GNU 15.1.1
</span></span><span class=line><span class=cl>-- Detecting C compiler ABI info
</span></span><span class=line><span class=cl>-- Detecting C compiler ABI info - <span class=k>done</span>
</span></span><span class=line><span class=cl>-- Check <span class=k>for</span> working C compiler: /usr/bin/cc - skipped
</span></span><span class=line><span class=cl>-- Detecting C compile features
</span></span><span class=line><span class=cl>-- Detecting C compile features - <span class=k>done</span>
</span></span><span class=line><span class=cl>-- Detecting CXX compiler ABI info
</span></span><span class=line><span class=cl>-- Detecting CXX compiler ABI info - <span class=k>done</span>
</span></span><span class=line><span class=cl>-- Check <span class=k>for</span> working CXX compiler: /usr/bin/c++ - skipped
</span></span><span class=line><span class=cl>-- Detecting CXX compile features
</span></span><span class=line><span class=cl>-- Detecting CXX compile features - <span class=k>done</span>
</span></span><span class=line><span class=cl>-- Found Git: /usr/bin/git <span class=o>(</span>found version <span class=s2>&#34;2.49.0&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
</span></span><span class=line><span class=cl>-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
</span></span><span class=line><span class=cl>-- Found Threads: TRUE
</span></span><span class=line><span class=cl>-- CMAKE_SYSTEM_PROCESSOR: x86_64
</span></span><span class=line><span class=cl>-- x86 detected
</span></span><span class=line><span class=cl>-- Configuring <span class=k>done</span> <span class=o>(</span>0.6s<span class=o>)</span>
</span></span><span class=line><span class=cl>-- Generating <span class=k>done</span> <span class=o>(</span>0.0s<span class=o>)</span>
</span></span><span class=line><span class=cl>-- Build files have been written to: /home/spgtty/edu/ggml/build
</span></span></code></pre></div><p>Компилираме</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>❯ make
</span></span><span class=line><span class=cl><span class=o>[</span>  4%<span class=o>]</span> Building C object src/CMakeFiles/ggml.dir/ggml.c.o
</span></span><span class=line><span class=cl><span class=o>[</span>  8%<span class=o>]</span> Linking C static library libggml.a
</span></span><span class=line><span class=cl><span class=o>[</span>  8%<span class=o>]</span> Built target ggml
</span></span><span class=line><span class=cl><span class=o>[</span> 12%<span class=o>]</span> Building C object tests/CMakeFiles/test-vec0.dir/test-vec0.c.o
</span></span><span class=line><span class=cl><span class=o>[</span> 16%<span class=o>]</span> Linking C executable ../bin/test-vec0
</span></span><span class=line><span class=cl><span class=o>[</span> 16%<span class=o>]</span> Built target test-vec0
</span></span><span class=line><span class=cl><span class=o>[</span> 20%<span class=o>]</span> Building C object tests/CMakeFiles/test-vec1.dir/test-vec1.c.o
</span></span><span class=line><span class=cl><span class=o>[</span> 25%<span class=o>]</span> Linking C executable ../bin/test-vec1
</span></span><span class=line><span class=cl><span class=o>[</span> 25%<span class=o>]</span> Built target test-vec1
</span></span><span class=line><span class=cl><span class=o>[</span> 29%<span class=o>]</span> Building C object tests/CMakeFiles/test-grad0.dir/test-grad0.c.o
</span></span><span class=line><span class=cl><span class=o>[</span> 33%<span class=o>]</span> Linking C executable ../bin/test-grad0
</span></span><span class=line><span class=cl><span class=o>[</span> 33%<span class=o>]</span> Built target test-grad0
</span></span><span class=line><span class=cl><span class=o>[</span> 37%<span class=o>]</span> Building C object tests/CMakeFiles/test-mul-mat0.dir/test-mul-mat0.c.o
</span></span><span class=line><span class=cl><span class=o>[</span> 41%<span class=o>]</span> Linking C executable ../bin/test-mul-mat0
</span></span><span class=line><span class=cl><span class=o>[</span> 41%<span class=o>]</span> Built target test-mul-mat0
</span></span><span class=line><span class=cl><span class=o>[</span> 45%<span class=o>]</span> Building C object tests/CMakeFiles/test0.dir/test0.c.o
</span></span><span class=line><span class=cl><span class=o>[</span> 50%<span class=o>]</span> Linking C executable ../bin/test0
</span></span><span class=line><span class=cl><span class=o>[</span> 50%<span class=o>]</span> Built target test0
</span></span><span class=line><span class=cl><span class=o>[</span> 54%<span class=o>]</span> Building C object tests/CMakeFiles/test1.dir/test1.c.o
</span></span><span class=line><span class=cl><span class=o>[</span> 58%<span class=o>]</span> Linking C executable ../bin/test1
</span></span><span class=line><span class=cl><span class=o>[</span> 58%<span class=o>]</span> Built target test1
</span></span><span class=line><span class=cl><span class=o>[</span> 62%<span class=o>]</span> Building C object tests/CMakeFiles/test2.dir/test2.c.o
</span></span><span class=line><span class=cl><span class=o>[</span> 66%<span class=o>]</span> Linking C executable ../bin/test2
</span></span><span class=line><span class=cl><span class=o>[</span> 66%<span class=o>]</span> Built target test2
</span></span><span class=line><span class=cl><span class=o>[</span> 70%<span class=o>]</span> Building C object tests/CMakeFiles/test3.dir/test3.c.o
</span></span><span class=line><span class=cl><span class=o>[</span> 75%<span class=o>]</span> Linking C executable ../bin/test3
</span></span><span class=line><span class=cl><span class=o>[</span> 75%<span class=o>]</span> Built target test3
</span></span><span class=line><span class=cl><span class=o>[</span> 79%<span class=o>]</span> Building CXX object examples/CMakeFiles/ggml_utils.dir/utils.cpp.o
</span></span><span class=line><span class=cl><span class=o>[</span> 83%<span class=o>]</span> Linking CXX static library libggml_utils.a
</span></span><span class=line><span class=cl><span class=o>[</span> 83%<span class=o>]</span> Built target ggml_utils
</span></span><span class=line><span class=cl><span class=o>[</span> 87%<span class=o>]</span> Building CXX object examples/gpt-2/CMakeFiles/gpt-2.dir/main.cpp.o
</span></span><span class=line><span class=cl><span class=o>[</span> 91%<span class=o>]</span> Linking CXX executable ../../bin/gpt-2
</span></span><span class=line><span class=cl><span class=o>[</span> 91%<span class=o>]</span> Built target gpt-2
</span></span><span class=line><span class=cl><span class=o>[</span> 95%<span class=o>]</span> Building CXX object examples/gpt-j/CMakeFiles/gpt-j.dir/main.cpp.o
</span></span><span class=line><span class=cl><span class=o>[</span>100%<span class=o>]</span> Linking CXX executable ../../bin/gpt-j
</span></span><span class=line><span class=cl><span class=o>[</span>100%<span class=o>]</span> Built target gpt-j
</span></span></code></pre></div><p>Всичко е наред, дори в първия commit вече има примери за gpt-2 и gpt-j. Страхотно!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>ggml/
</span></span><span class=line><span class=cl>├── include/ggml/ggml.h # Public API definitions
</span></span><span class=line><span class=cl>├── src/ggml.c # Core implementation
</span></span><span class=line><span class=cl>...
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>❯ <span class=nb>cd</span> ..
</span></span><span class=line><span class=cl>❯ mkdir playground
</span></span><span class=line><span class=cl>❯ ls
</span></span><span class=line><span class=cl>build  ggml  playground
</span></span><span class=line><span class=cl>❯ <span class=nb>cd</span> playground
</span></span><span class=line><span class=cl>❯ gcc main.c -I../ggml/include/ggml -L../build/src -lggml -lm -o main
</span></span></code></pre></div><h2 id=основни-примитиви>Основни примитиви</h2><h2 id=памет>Памет</h2><h3 id=тензор>Тензор</h3><p>Нека започнем със стандартния за всички ML библиотеки примитив - тензора. В нашия бъдещ граф на изчисленията - това са възлите.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>enum</span> <span class=n>ggml_type</span> <span class=n>type</span><span class=p>;</span> <span class=c1>// Data type (F32, F16, etc.)
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=kt>int</span> <span class=n>n_dims</span><span class=p>;</span> <span class=c1>// Number of dimensions
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=n>ne</span><span class=p>[</span><span class=n>GGML_MAX_DIMS</span><span class=p>];</span> <span class=c1>// Number of elements per dimension
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>size_t</span> <span class=n>nb</span><span class=p>[</span><span class=n>GGML_MAX_DIMS</span><span class=p>];</span> <span class=c1>// Number of bytes (stride)
</span></span></span><span class=line><span class=cl><span class=c1></span>                              <span class=c1>// nb[0] = sizeof(type)
</span></span></span><span class=line><span class=cl><span class=c1></span>                              <span class=c1>// nb[1] = nb[0]   * ne[0] + padding
</span></span></span><span class=line><span class=cl><span class=c1></span>                              <span class=c1>// nb[i] = nb[i-1] * ne[i-1]
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=k>enum</span> <span class=n>ggml_op</span> <span class=n>op</span><span class=p>;</span> <span class=c1>// Operation that created this tensor
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>bool</span> <span class=n>is_param</span><span class=p>;</span> <span class=c1>// Is this a parameter (for gradients)
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>grad</span><span class=p>;</span> <span class=c1>// Gradient tensor
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>src0</span><span class=p>;</span> <span class=c1>// First source tensor
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>src1</span><span class=p>;</span> <span class=c1>// Second source tensor
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=kt>void</span> <span class=o>*</span> <span class=n>data</span><span class=p>;</span> <span class=c1>// Actual data pointer
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>};</span>
</span></span></code></pre></div><p>type - това е enum, означаващ типа данни,<br>n_dims - размерност (0-скалар, 1 вектор, 2 - матрица и т.н.)<br>ne - масив, съдържащ броя елементи в дадената размерност (например за матрица 3x4 - ne[0]=4 cols, ne[1]=3 rows)<br>nb - стъпка на размерността в байтове, т.е. през колко байта започва новия елемент от размерността. За матрица 3х4 (3 реда и 4 колони) с данни F32 (4 байта на променлива) получаваме че nb[0] = 4 (размера на float), nb[1] = 4х4=16 байта в реда. Ако елементите в реда не са подравнени, трябва да се добави padding.<br>Защо не можем просто да изчислим nb на базата на размера на типа данни и размерността? Това е направено, за да можем да работим с проекции на по-малки размерности в рамките на дадения тензор (т.е. представете си че имате картинка rgb и правите свъртка (convolution) - трябва ни да работим с kernel 16на16 в рамките на по-голям размер, няма да ни се налага да копираме данни, ще можем да използваме nb за да намираме следващия ред). Друг аспект е подравняването на данните, за да всеки нов ред започва от адрес кратен на <code>#define GGML_MEM_ALIGN 16</code></p><p>op - enum описващ операцията във възела (0 - NOOP ако тензорът просто съдържа данни).<br>is_param - флаг определя дали трябва да се изчислява градиента в дадения възел (грубо казано съдържа ли тензорът X или θ, аналог на torch.no_grad())</p><p>след това следва указател към друг тензор, в който се записват стойностите на градиентите,<br>src0, src1 - родителските възли в нашия граф на изчисленията</p><p>data - указател към областта от паметта съдържаща данните</p><h3 id=контекст>Контекст</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>struct</span> <span class=n>ggml_context</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=kt>size_t</span> <span class=n>mem_size</span><span class=p>;</span> <span class=c1>// Total memory size
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>void</span> <span class=o>*</span> <span class=n>mem_buffer</span><span class=p>;</span> <span class=c1>// Memory buffer
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=kt>bool</span> <span class=n>mem_buffer_owned</span><span class=p>;</span> <span class=c1>// Whether we own the buffer
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=kt>int</span> <span class=n>n_objects</span><span class=p>;</span> <span class=c1>// Number of allocated objects
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>	<span class=k>struct</span> <span class=n>ggml_object</span> <span class=o>*</span> <span class=n>objects_begin</span><span class=p>;</span> <span class=c1>// Linked list of objects
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=k>struct</span> <span class=n>ggml_object</span> <span class=o>*</span> <span class=n>objects_end</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><p>Контекстът в ggml е областта от паметта, в която работим. Идеята е да не заделяме памет под всеки тензор, като извикваме malloc много пъти, а вместо това да заделим голяма област от паметта и да работим в нея, назначавайки области от тази памет под тензорите.</p><p>Структурата е проста<br>size_t - размер на паметта<br>mem_buffer - адресът на началото на блока памет. Можем да го оставим празен, тогава ggml ще задели памет за нас с помощта на malloc при инициализацията</p><p>mem_buffer_owned - флаг показва кой е заделил паметта, ggml или ние, за да разберем кой трябва да я освободи</p><p>След това простo свързано дърво от ggml обекти,<br>n_objects<br>objects_begin<br>objects_end</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>struct</span> <span class=n>ggml_object</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>size_t</span> <span class=n>offset</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>size_t</span> <span class=n>size</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_object</span> <span class=o>*</span> <span class=n>next</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>char</span> <span class=n>padding</span><span class=p>[</span><span class=mi>8</span><span class=p>];</span> <span class=c1>//Explicit padding (not used for data)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>};</span>
</span></span></code></pre></div><p>Всеки възел съдържа отместване спрямо паметта на контекста, където се намира нашия тензор. Така, за да преминем от обект към тензор, ни трябва:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#define GGML_OBJECT_TO_TENSOR(obj) \
</span></span></span><span class=line><span class=cl><span class=cp>((struct ggml_tensor *) ((char *) ctx-&gt;mem_buffer + obj-&gt;offset))
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=k>struct</span> <span class=n>ggml_object</span> <span class=o>*</span> <span class=n>obj</span> <span class=o>=</span> <span class=nf>find_some_object</span><span class=p>(</span><span class=n>ctx</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span> <span class=o>=</span> <span class=nf>GGML_OBJECT_TO_TENSOR</span><span class=p>(</span><span class=n>obj</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>float</span> <span class=o>*</span> <span class=n>actual_data</span> <span class=o>=</span> <span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=p>)</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>data</span><span class=p>;</span>
</span></span></code></pre></div><h3 id=подравняване>Подравняване</h3><p>Отделно си заслужава да разгледаме подравняването на данните. Тъй като библиотеката използва SIMD (Single Instruction Multiple Data) инструкции на процесора, което позволява за времето на една операция да се провежда тази операция паралелно върху масиви от данни, то това налага определени изисквания към адресирането на данните, а именно те трябва да бъдат подравнени, т.е. да се намират на адрес кратен на 16.</p><p>GGML реализира това така -</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// align to GGML_MEM_ALIGN
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>size_needed</span> <span class=o>=</span> <span class=p>((</span><span class=n>size_needed</span> <span class=o>+</span> <span class=n>GGML_MEM_ALIGN</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span><span class=o>/</span><span class=n>GGML_MEM_ALIGN</span><span class=p>)</span><span class=o>*</span><span class=n>GGML_MEM_ALIGN</span><span class=p>;</span>
</span></span></code></pre></div><p>Ще разгледаме по-късно как се създава тензор, там се използва този код.</p><p>Подравняването ни позволява да бъдем сигурни че данните лежат в нужните ни адреси</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Context Memory Buffer:
</span></span><span class=line><span class=cl>┌───────────────────────────────────────────────────────────┐
</span></span><span class=line><span class=cl>│ ctx-&gt;mem_buffer                                           │
</span></span><span class=line><span class=cl>├──────────┬──────────────┬──────────────────┬──────────────┤
</span></span><span class=line><span class=cl>│ object1  │ tensor1      │ data1 (aligned)  │ object2      │
</span></span><span class=line><span class=cl>│ metadata │ (aligned)    │ [1.0,2.0,3.0]    │ metadata     │
</span></span><span class=line><span class=cl>├──────────┼──────────────┼──────────────────┼──────────────┤
</span></span><span class=line><span class=cl>│ offset=0 │ offset=16    │ offset=64        │ offset=144   │
</span></span><span class=line><span class=cl>│          │ (padded)     │ (padded)         │ (padded)     │
</span></span><span class=line><span class=cl>└──────────┴──────────────┴──────────────────┴──────────────┘
</span></span></code></pre></div><h3 id=типове-данни>Типове данни</h3><p>GGML поддържа следните типове данни</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>enum</span> <span class=n>ggml_type</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>GGML_TYPE_I8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>GGML_TYPE_I16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>GGML_TYPE_I32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>GGML_TYPE_F16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>GGML_TYPE_F32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>GGML_TYPE_COUNT</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><p>Особено внимание заслужава F16 - типа с половинна точност от стандартния Float 32. Но този тип не се поддържа в x86, затова всички изчисления над този тип първо превеждат данните в F32, а после обратно</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=k>typedef</span> <span class=kt>uint16_t</span> <span class=kt>ggml_fp16_t</span><span class=p>;</span> <span class=c1>// Just a 16-bit integer!
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=cp>#ifdef __ARM_NEON
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=c1>// we use the built-in 16-bit float type
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>typedef</span> <span class=n>__fp16</span> <span class=kt>ggml_fp16_t</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#else
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>typedef</span> <span class=kt>uint16_t</span> <span class=kt>ggml_fp16_t</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#endif
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>float</span> <span class=nf>ggml_fp16_to_fp32</span><span class=p>(</span><span class=kt>ggml_fp16_t</span> <span class=n>x</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=kt>ggml_fp16_t</span> <span class=nf>ggml_fp32_to_fp16</span><span class=p>(</span><span class=kt>float</span> <span class=n>x</span><span class=p>);</span>
</span></span></code></pre></div><p>Представянето на FP16 се различава от представянето на FP32 само с броя битове за експонентата и мантисата:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>// FP16 (16 bits total):
</span></span><span class=line><span class=cl>// ┌─┬─────────┬──────────┐
</span></span><span class=line><span class=cl>// │S│EEEEE    │MMMMMMMMMM│
</span></span><span class=line><span class=cl>// └─┴─────────┴──────────┘
</span></span><span class=line><span class=cl>//  1  5 bits    10 bits
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>// FP32 (32 bits total):
</span></span><span class=line><span class=cl>// ┌─┬────────────┬───────────────────────┐
</span></span><span class=line><span class=cl>// │S│EEEEEEEE    │MMMMMMMMMMMMMMMMMMMMMMM│
</span></span><span class=line><span class=cl>// └─┴────────────┴───────────────────────┘
</span></span><span class=line><span class=cl>//  1  8 bits          23 bits
</span></span></code></pre></div><p>В тази статия няма да разглеждам функциите за превод от един тип в друг, тъй като за това ми трябва да напиша още една статия. Не можем просто да преместим битовете, тъй като различните типове имат различна базова стойност на експонентата.<br>Също така трябва да се вземат предвид специалните нормализирани стойности.</p><p>Ето как изглежда реализацията на конвертирането:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#ifdef __ARM_NEON
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;arm_neon.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=kt>float</span> <span class=nf>ggml_fp16_to_fp32</span><span class=p>(</span><span class=kt>ggml_fp16_t</span> <span class=n>x</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kt>ggml_fp16_t</span> <span class=nf>ggml_fp32_to_fp16</span><span class=p>(</span><span class=kt>float</span> <span class=n>x</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=cp>#else
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;immintrin.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=k>static</span> <span class=kr>inline</span> <span class=kt>float</span> <span class=nf>fp32_from_bits</span><span class=p>(</span><span class=kt>uint32_t</span> <span class=n>w</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>union</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>uint32_t</span> <span class=n>as_bits</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>as_value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=n>fp32</span> <span class=o>=</span> <span class=p>{</span> <span class=n>w</span> <span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>fp32</span><span class=p>.</span><span class=n>as_value</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>static</span> <span class=kr>inline</span> <span class=kt>uint32_t</span> <span class=nf>fp32_to_bits</span><span class=p>(</span><span class=kt>float</span> <span class=n>f</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>union</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=kt>float</span> <span class=n>as_value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>		<span class=kt>uint32_t</span> <span class=n>as_bits</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span> <span class=n>fp32</span> <span class=o>=</span> <span class=p>{</span> <span class=n>f</span> <span class=p>};</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>fp32</span><span class=p>.</span><span class=n>as_bits</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>float</span> <span class=nf>ggml_fp16_to_fp32</span><span class=p>(</span><span class=kt>ggml_fp16_t</span> <span class=n>h</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>w</span> <span class=o>=</span> <span class=p>(</span><span class=kt>uint32_t</span><span class=p>)</span> <span class=n>h</span> <span class=o>&lt;&lt;</span> <span class=mi>16</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>sign</span> <span class=o>=</span> <span class=n>w</span> <span class=o>&amp;</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x80000000</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>two_w</span> <span class=o>=</span> <span class=n>w</span> <span class=o>+</span> <span class=n>w</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>exp_offset</span> <span class=o>=</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0xE0</span><span class=p>)</span> <span class=o>&lt;&lt;</span> <span class=mi>23</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#if defined(__STDC_VERSION__) &amp;&amp; (__STDC_VERSION__ &gt;= 199901L) || defined(__GNUC__) &amp;&amp; !defined(__STRICT_ANSI__)
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>const</span> <span class=kt>float</span> <span class=n>exp_scale</span> <span class=o>=</span> <span class=mh>0x1</span><span class=mf>.0</span><span class=n>p</span><span class=o>-</span><span class=mf>112f</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#else
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>const</span> <span class=kt>float</span> <span class=n>exp_scale</span> <span class=o>=</span> <span class=nf>fp32_from_bits</span><span class=p>(</span><span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x7800000</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=cp>#endif
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>const</span> <span class=kt>float</span> <span class=n>normalized_value</span> <span class=o>=</span> <span class=nf>fp32_from_bits</span><span class=p>((</span><span class=n>two_w</span> <span class=o>&gt;&gt;</span> <span class=mi>4</span><span class=p>)</span> <span class=o>+</span> <span class=n>exp_offset</span><span class=p>)</span> <span class=o>*</span> <span class=n>exp_scale</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>magic_mask</span> <span class=o>=</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mi>126</span><span class=p>)</span> <span class=o>&lt;&lt;</span> <span class=mi>23</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>float</span> <span class=n>magic_bias</span> <span class=o>=</span> <span class=mf>0.5f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>float</span> <span class=n>denormalized_value</span> <span class=o>=</span> <span class=nf>fp32_from_bits</span><span class=p>((</span><span class=n>two_w</span> <span class=o>&gt;&gt;</span> <span class=mi>17</span><span class=p>)</span> <span class=o>|</span> <span class=n>magic_mask</span><span class=p>)</span> <span class=o>-</span> <span class=n>magic_bias</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>denormalized_cutoff</span> <span class=o>=</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span> <span class=o>&lt;&lt;</span> <span class=mi>27</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>result</span> <span class=o>=</span> <span class=n>sign</span> <span class=o>|</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=n>two_w</span> <span class=o>&lt;</span> <span class=n>denormalized_cutoff</span> <span class=o>?</span> <span class=nf>fp32_to_bits</span><span class=p>(</span><span class=n>denormalized_value</span><span class=p>)</span> <span class=o>:</span> <span class=nf>fp32_to_bits</span><span class=p>(</span><span class=n>normalized_value</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nf>fp32_from_bits</span><span class=p>(</span><span class=n>result</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>ggml_fp16_t</span> <span class=nf>ggml_fp32_to_fp16</span><span class=p>(</span><span class=kt>float</span> <span class=n>f</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=cp>#if defined(__STDC_VERSION__) &amp;&amp; (__STDC_VERSION__ &gt;= 199901L) || defined(__GNUC__) &amp;&amp; !defined(__STRICT_ANSI__)
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>const</span> <span class=kt>float</span> <span class=n>scale_to_inf</span> <span class=o>=</span> <span class=mh>0x1</span><span class=mf>.0</span><span class=n>p</span><span class=o>+</span><span class=mf>112f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>float</span> <span class=n>scale_to_zero</span> <span class=o>=</span> <span class=mh>0x1</span><span class=mf>.0</span><span class=n>p</span><span class=o>-</span><span class=mf>110f</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=cp>#else
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=k>const</span> <span class=kt>float</span> <span class=n>scale_to_inf</span> <span class=o>=</span> <span class=nf>fp32_from_bits</span><span class=p>(</span><span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x77800000</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>float</span> <span class=n>scale_to_zero</span> <span class=o>=</span> <span class=nf>fp32_from_bits</span><span class=p>(</span><span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x08800000</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=cp>#endif
</span></span></span><span class=line><span class=cl><span class=cp></span>    <span class=kt>float</span> <span class=n>base</span> <span class=o>=</span> <span class=p>(</span><span class=nf>fabsf</span><span class=p>(</span><span class=n>f</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale_to_inf</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale_to_zero</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>w</span> <span class=o>=</span> <span class=nf>fp32_to_bits</span><span class=p>(</span><span class=n>f</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>shl1_w</span> <span class=o>=</span> <span class=n>w</span> <span class=o>+</span> <span class=n>w</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>sign</span> <span class=o>=</span> <span class=n>w</span> <span class=o>&amp;</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x80000000</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=kt>uint32_t</span> <span class=n>bias</span> <span class=o>=</span> <span class=n>shl1_w</span> <span class=o>&amp;</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0xFF000000</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>bias</span> <span class=o>&lt;</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x71000000</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>bias</span> <span class=o>=</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x71000000</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>base</span> <span class=o>=</span> <span class=nf>fp32_from_bits</span><span class=p>((</span><span class=n>bias</span> <span class=o>&gt;&gt;</span> <span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x07800000</span><span class=p>))</span> <span class=o>+</span> <span class=n>base</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>bits</span> <span class=o>=</span> <span class=nf>fp32_to_bits</span><span class=p>(</span><span class=n>base</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>exp_bits</span> <span class=o>=</span> <span class=p>(</span><span class=n>bits</span> <span class=o>&gt;&gt;</span> <span class=mi>13</span><span class=p>)</span> <span class=o>&amp;</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x00007C00</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>mantissa_bits</span> <span class=o>=</span> <span class=n>bits</span> <span class=o>&amp;</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0x00000FFF</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>uint32_t</span> <span class=n>nonsign</span> <span class=o>=</span> <span class=n>exp_bits</span> <span class=o>+</span> <span class=n>mantissa_bits</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=n>sign</span> <span class=o>&gt;&gt;</span> <span class=mi>16</span><span class=p>)</span> <span class=o>|</span> <span class=p>(</span><span class=n>shl1_w</span> <span class=o>&gt;</span> <span class=nf>UINT32_C</span><span class=p>(</span><span class=mh>0xFF000000</span><span class=p>)</span> <span class=o>?</span> <span class=nf>UINT16_C</span><span class=p>(</span><span class=mh>0x7E00</span><span class=p>)</span> <span class=o>:</span> <span class=n>nonsign</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=cp>#endif
</span></span></span></code></pre></div><h3 id=оптимизации>Оптимизации</h3><p>Интересно е, че дори в първия commit вече присъстват оптимизации. Това е много яко.</p><p>От това, което ми направи впечатление - това е padding-а за променливите, т.е. разделянето на променливите в паметта, за да се избегне копирането на кеша от ядро в ядро<br><code>cgraph->work_size = work_size + CACHE_LINE_SIZE*(n_threads - 1);</code></p><p>Също така в кода присъстват SIMD оптимизации, за паралелно изчисляване на математически операции, например -</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=cp>#else </span><span class=c1>// x86 AVX implementation
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=kt>int</span> <span class=n>n32</span> <span class=o>=</span> <span class=mi>32</span><span class=o>*</span><span class=p>(</span><span class=n>n</span><span class=o>/</span><span class=mi>32</span><span class=p>);</span> <span class=c1>// Process 32 elements at a time
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=n>__m256</span> <span class=n>sum0</span> <span class=o>=</span> <span class=nf>_mm256_setzero_ps</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=n>__m256</span> <span class=n>sum1</span> <span class=o>=</span> <span class=nf>_mm256_setzero_ps</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=n>__m256</span> <span class=n>sum2</span> <span class=o>=</span> <span class=nf>_mm256_setzero_ps</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=n>__m256</span> <span class=n>sum3</span> <span class=o>=</span> <span class=nf>_mm256_setzero_ps</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n32</span><span class=p>;</span> <span class=n>i</span> <span class=o>+=</span> <span class=mi>32</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=c1>// Convert F16 to F32 and load
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>__m256</span> <span class=n>x0</span> <span class=o>=</span> <span class=nf>_mm256_cvtph_ps</span><span class=p>(</span><span class=nf>_mm_loadu_si128</span><span class=p>((</span><span class=kr>__m128i</span><span class=o>*</span><span class=p>)(</span><span class=n>x</span> <span class=o>+</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>0</span><span class=p>)));</span>
</span></span><span class=line><span class=cl>	<span class=n>__m256</span> <span class=n>y0</span> <span class=o>=</span> <span class=nf>_mm256_cvtph_ps</span><span class=p>(</span><span class=nf>_mm_loadu_si128</span><span class=p>((</span><span class=kr>__m128i</span><span class=o>*</span><span class=p>)(</span><span class=n>y</span> <span class=o>+</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>0</span><span class=p>)));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1>// Fused multiply-add
</span></span></span><span class=line><span class=cl><span class=c1></span>	<span class=n>sum0</span> <span class=o>=</span> <span class=nf>_mm256_fmadd_ps</span><span class=p>(</span><span class=n>x0</span><span class=p>,</span> <span class=n>y0</span><span class=p>,</span> <span class=n>sum0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>	<span class=c1>// ... process remaining vectors
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=cp>#endif
</span></span></span></code></pre></div><p>Няма да се спирам на оптимизациите, но трябва да се разбира, че благодарение на това библиотеката работи бързо, използвайки предимствата на платформата, на която е стартирана.</p><h2 id=тензори-в-паметта>Тензори в паметта</h2><p>За удобство GGML има няколко полезни функции, използвани при изчисленията. По-долу са представени съкратени извадки от кода.</p><h3 id=проверка-на-размерността>Проверка на размерността</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// From ggml.c - determine tensor dimensionality
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>bool</span> <span class=nf>ggml_is_scalar</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span> <span class=o>&amp;&amp;</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span> <span class=o>&amp;&amp;</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span> <span class=o>&amp;&amp;</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kt>bool</span> <span class=nf>ggml_is_vector</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span> <span class=o>&amp;&amp;</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span> <span class=o>&amp;&amp;</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kt>bool</span> <span class=nf>ggml_is_matrix</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span> <span class=o>&amp;&amp;</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=o>==</span> <span class=mi>1</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=проверка-на-смежност--padding>Проверка на смежност / padding</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>bool</span> <span class=nf>ggml_is_contiguous</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span>
</span></span><span class=line><span class=cl>		<span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=n>GGML_TYPE_SIZE</span><span class=p>[</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>type</span><span class=p>]</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>		<span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>==</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>		<span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>		<span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=o>==</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Функцията проверява</p><ol><li>Елементите и редовете са плътно опаковани в паметта (няма padding-ове)</li><li>Всяка размерност умножена по броя елементи в нея - това е стъпката на следващата размерност.</li></ol><h3 id=проверка-на-смежност-без-първото-измерение>Проверка на смежност без първото измерение</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>bool</span> <span class=nf>ggml_is_padded_1d</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span>
</span></span><span class=line><span class=cl>		<span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=n>GGML_TYPE_SIZE</span><span class=p>[</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>type</span><span class=p>]</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>		<span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span> <span class=o>==</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>&amp;&amp;</span>
</span></span><span class=line><span class=cl>		<span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>3</span><span class=p>]</span> <span class=o>==</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>2</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Същото като по-горе, но без проверка на padding-а по първото измерение.</p><p>Защо ни трябва такава функция?</p><ul><li><strong>SIMD операции</strong>: Изискват <code>ggml_is_contiguous() == true</code></li><li>Операции по редове: Достатъчно е <code>ggml_is_padded_1d() == true</code></li><li>Произволни операции: Трябва да работят при всякакъв padding</li></ul><p>НО - в първата версия, padding-ът за редовете отсъства в имплементацията за създаване на нов тензор, затова тази и предишната функция винаги връщат true, освен ако не създавате тензори сами.</p><h3 id=размер-на-тензорите>Размер на тензорите</h3><p>Коментарите са излишни, кодът е ясен</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kt>int</span> <span class=nf>ggml_nelements</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>3</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>ggml_nrows</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>*</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>3</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>size_t</span> <span class=nf>ggml_nbytes</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>tensor</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=nf>ggml_nelements</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span><span class=o>*</span><span class=n>GGML_TYPE_SIZE</span><span class=p>[</span><span class=n>tensor</span><span class=o>-&gt;</span><span class=n>type</span><span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=създаване-на-тензори>Създаване на тензори</h2><p>Преди да преминем към разглеждането на имплементацията на функцията за създаване на тензори, нека опитаме да напишем няколко примера и да проверим че всичко работи както е замислено.</p><details><summary><b>Пример за работа с тензори</b></summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;ggml.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>print_tensor_info</span><span class=p>(</span><span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span><span class=o>*</span> <span class=n>t</span><span class=p>,</span> <span class=k>const</span> <span class=kt>char</span><span class=o>*</span> <span class=n>name</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>=== %s ===</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>name</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Type: %d, Dims: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>type</span><span class=p>,</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>n_dims</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Shape (ne): [%d, %d, %d, %d]</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>3</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Strides (nb): [%zu, %zu, %zu, %zu]</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>2</span><span class=p>],</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>3</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Elements: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_nelements</span><span class=p>(</span><span class=n>t</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Rows: %d</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_nrows</span><span class=p>(</span><span class=n>t</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Bytes: %zu</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_nbytes</span><span class=p>(</span><span class=n>t</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Properties:</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;  Scalar: %s</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_is_scalar</span><span class=p>(</span><span class=n>t</span><span class=p>)</span> <span class=o>?</span> <span class=s>&#34;YES&#34;</span> <span class=o>:</span> <span class=s>&#34;NO&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;  Vector: %s</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_is_vector</span><span class=p>(</span><span class=n>t</span><span class=p>)</span> <span class=o>?</span> <span class=s>&#34;YES&#34;</span> <span class=o>:</span> <span class=s>&#34;NO&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;  Matrix: %s</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_is_matrix</span><span class=p>(</span><span class=n>t</span><span class=p>)</span> <span class=o>?</span> <span class=s>&#34;YES&#34;</span> <span class=o>:</span> <span class=s>&#34;NO&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;  Contiguous: %s</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_is_contiguous</span><span class=p>(</span><span class=n>t</span><span class=p>)</span> <span class=o>?</span> <span class=s>&#34;YES&#34;</span> <span class=o>:</span> <span class=s>&#34;NO&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;  Padded 1D: %s</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_is_padded_1d</span><span class=p>(</span><span class=n>t</span><span class=p>)</span> <span class=o>?</span> <span class=s>&#34;YES&#34;</span> <span class=o>:</span> <span class=s>&#34;NO&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_init_params</span> <span class=n>params</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=p>.</span><span class=n>mem_size</span> <span class=o>=</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>10</span><span class=p>,</span> <span class=c1>// 10 MB
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=p>.</span><span class=n>mem_buffer</span> <span class=o>=</span> <span class=nb>NULL</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_context</span><span class=o>*</span> <span class=n>ctx</span> <span class=o>=</span> <span class=nf>ggml_init</span><span class=p>(</span><span class=n>params</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Create different tensor shapes
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>struct</span> <span class=n>ggml_tensor</span><span class=o>*</span> <span class=n>scalar</span> <span class=o>=</span> <span class=nf>ggml_new_tensor_1d</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>GGML_TYPE_F32</span><span class=p>,</span> <span class=mi>1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span><span class=o>*</span> <span class=n>vector</span> <span class=o>=</span> <span class=nf>ggml_new_tensor_1d</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>GGML_TYPE_F32</span><span class=p>,</span> <span class=mi>100</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span><span class=o>*</span> <span class=n>matrix</span> <span class=o>=</span> <span class=nf>ggml_new_tensor_2d</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>GGML_TYPE_F32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span><span class=o>*</span> <span class=n>tensor3d</span> <span class=o>=</span> <span class=nf>ggml_new_tensor_3d</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>GGML_TYPE_F16</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>28</span><span class=p>,</span> <span class=mi>64</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span><span class=o>*</span> <span class=n>tensor4d</span> <span class=o>=</span> <span class=nf>ggml_new_tensor_4d</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>GGML_TYPE_F32</span><span class=p>,</span> <span class=mi>224</span><span class=p>,</span> <span class=mi>224</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>16</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>print_tensor_info</span><span class=p>(</span><span class=n>scalar</span><span class=p>,</span> <span class=s>&#34;Scalar&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>print_tensor_info</span><span class=p>(</span><span class=n>vector</span><span class=p>,</span> <span class=s>&#34;Vector&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>print_tensor_info</span><span class=p>(</span><span class=n>matrix</span><span class=p>,</span> <span class=s>&#34;Matrix&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>print_tensor_info</span><span class=p>(</span><span class=n>tensor3d</span><span class=p>,</span> <span class=s>&#34;3D Tensor&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>print_tensor_info</span><span class=p>(</span><span class=n>tensor4d</span><span class=p>,</span> <span class=s>&#34;4D Tensor (Batch)&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>ggml_free</span><span class=p>(</span><span class=n>ctx</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>=== Scalar ===
</span></span><span class=line><span class=cl>Type: 4, Dims: 1
</span></span><span class=line><span class=cl>Shape (ne): [1, 1, 1, 1]
</span></span><span class=line><span class=cl>Strides (nb): [4, 4, 4, 4]
</span></span><span class=line><span class=cl>Elements: 1
</span></span><span class=line><span class=cl>Rows: 1
</span></span><span class=line><span class=cl>Bytes: 4
</span></span><span class=line><span class=cl>Properties:
</span></span><span class=line><span class=cl>  Scalar: YES
</span></span><span class=line><span class=cl>  Vector: YES
</span></span><span class=line><span class=cl>  Matrix: YES
</span></span><span class=line><span class=cl>  Contiguous: YES
</span></span><span class=line><span class=cl>  Padded 1D: YES
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== Vector ===
</span></span><span class=line><span class=cl>Type: 4, Dims: 1
</span></span><span class=line><span class=cl>Shape (ne): [100, 1, 1, 1]
</span></span><span class=line><span class=cl>Strides (nb): [4, 400, 400, 400]
</span></span><span class=line><span class=cl>Elements: 100
</span></span><span class=line><span class=cl>Rows: 1
</span></span><span class=line><span class=cl>Bytes: 400
</span></span><span class=line><span class=cl>Properties:
</span></span><span class=line><span class=cl>  Scalar: NO
</span></span><span class=line><span class=cl>  Vector: YES
</span></span><span class=line><span class=cl>  Matrix: YES
</span></span><span class=line><span class=cl>  Contiguous: YES
</span></span><span class=line><span class=cl>  Padded 1D: YES
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== Matrix ===
</span></span><span class=line><span class=cl>Type: 4, Dims: 2
</span></span><span class=line><span class=cl>Shape (ne): [64, 32, 1, 1]
</span></span><span class=line><span class=cl>Strides (nb): [4, 256, 8192, 8192]
</span></span><span class=line><span class=cl>Elements: 2048
</span></span><span class=line><span class=cl>Rows: 32
</span></span><span class=line><span class=cl>Bytes: 8192
</span></span><span class=line><span class=cl>Properties:
</span></span><span class=line><span class=cl>  Scalar: NO
</span></span><span class=line><span class=cl>  Vector: NO
</span></span><span class=line><span class=cl>  Matrix: YES
</span></span><span class=line><span class=cl>  Contiguous: YES
</span></span><span class=line><span class=cl>  Padded 1D: YES
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== 3D Tensor ===
</span></span><span class=line><span class=cl>Type: 3, Dims: 3
</span></span><span class=line><span class=cl>Shape (ne): [28, 28, 64, 1]
</span></span><span class=line><span class=cl>Strides (nb): [2, 56, 1568, 100352]
</span></span><span class=line><span class=cl>Elements: 50176
</span></span><span class=line><span class=cl>Rows: 1792
</span></span><span class=line><span class=cl>Bytes: 100352
</span></span><span class=line><span class=cl>Properties:
</span></span><span class=line><span class=cl>  Scalar: NO
</span></span><span class=line><span class=cl>  Vector: NO
</span></span><span class=line><span class=cl>  Matrix: NO
</span></span><span class=line><span class=cl>  Contiguous: YES
</span></span><span class=line><span class=cl>  Padded 1D: YES
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== 4D Tensor (Batch) ===
</span></span><span class=line><span class=cl>Type: 4, Dims: 4
</span></span><span class=line><span class=cl>Shape (ne): [224, 224, 3, 16]
</span></span><span class=line><span class=cl>Strides (nb): [4, 896, 200704, 602112]
</span></span><span class=line><span class=cl>Elements: 2408448
</span></span><span class=line><span class=cl>Rows: 10752
</span></span><span class=line><span class=cl>Bytes: 9633792
</span></span><span class=line><span class=cl>Properties:
</span></span><span class=line><span class=cl>  Scalar: NO
</span></span><span class=line><span class=cl>  Vector: NO
</span></span><span class=line><span class=cl>  Matrix: NO
</span></span><span class=line><span class=cl>  Contiguous: YES
</span></span><span class=line><span class=cl>  Padded 1D: YES
</span></span><span class=line><span class=cl>ggml_free: context 0 with 5 objects has been freed. memory used = 9743552
</span></span></code></pre></div></details><br><p>Нека отобразим паметта</p><details><summary><b>Визуализация на паметта на тензора</b></summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;stdlib.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;ggml.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>visualize_2d_layout</span><span class=p>(</span><span class=k>struct</span> <span class=n>ggml_tensor</span><span class=o>*</span> <span class=n>t</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=nf>ggml_is_matrix</span><span class=p>(</span><span class=n>t</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Not a 2D matrix!</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>=== 2D Memory Layout ===</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Shape: %dx%d (width x height)</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Strides: [%zu, %zu] bytes</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Fill with sample data
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span> <span class=p>(</span><span class=n>t</span><span class=o>-&gt;</span><span class=n>type</span> <span class=o>==</span> <span class=n>GGML_TYPE_F32</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span><span class=o>*</span> <span class=n>data</span> <span class=o>=</span> <span class=p>(</span><span class=kt>float</span><span class=o>*</span><span class=p>)</span><span class=n>t</span><span class=o>-&gt;</span><span class=n>data</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=nf>ggml_nelements</span><span class=p>(</span><span class=n>t</span><span class=p>);</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>i</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>// Print logical layout
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>Logical view:</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>row</span> <span class=o>&lt;</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>];</span> <span class=n>row</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Row %d: &#34;</span><span class=p>,</span> <span class=n>row</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>col</span> <span class=o>&lt;</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span> <span class=n>col</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=c1>// Calculate memory offset
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=kt>size_t</span> <span class=n>offset</span> <span class=o>=</span> <span class=n>row</span> <span class=o>*</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>col</span> <span class=o>*</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                <span class=kt>float</span><span class=o>*</span> <span class=n>element</span> <span class=o>=</span> <span class=p>(</span><span class=kt>float</span><span class=o>*</span><span class=p>)((</span><span class=kt>char</span><span class=o>*</span><span class=p>)</span><span class=n>t</span><span class=o>-&gt;</span><span class=n>data</span> <span class=o>+</span> <span class=n>offset</span><span class=p>);</span>
</span></span><span class=line><span class=cl>                <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;%6.0f &#34;</span><span class=p>,</span> <span class=o>*</span><span class=n>element</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>// Print memory addresses
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>Memory addresses:</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>row</span> <span class=o>&lt;</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>1</span><span class=p>];</span> <span class=n>row</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Row %d: &#34;</span><span class=p>,</span> <span class=n>row</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>col</span> <span class=o>&lt;</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span> <span class=n>col</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=kt>size_t</span> <span class=n>offset</span> <span class=o>=</span> <span class=n>row</span> <span class=o>*</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>col</span> <span class=o>*</span> <span class=n>t</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;%6zu &#34;</span><span class=p>,</span> <span class=n>offset</span><span class=p>);</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_init_params</span> <span class=n>params</span> <span class=o>=</span> <span class=p>{</span> <span class=p>.</span><span class=n>mem_size</span> <span class=o>=</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span><span class=p>,</span> <span class=p>.</span><span class=n>mem_buffer</span> <span class=o>=</span> <span class=nb>NULL</span> <span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_context</span><span class=o>*</span> <span class=n>ctx</span> <span class=o>=</span> <span class=nf>ggml_init</span><span class=p>(</span><span class=n>params</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Create a small matrix for visualization
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>struct</span> <span class=n>ggml_tensor</span><span class=o>*</span> <span class=n>matrix</span> <span class=o>=</span> <span class=nf>ggml_new_tensor_2d</span><span class=p>(</span><span class=n>ctx</span><span class=p>,</span> <span class=n>GGML_TYPE_F32</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>visualize_2d_layout</span><span class=p>(</span><span class=n>matrix</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>ggml_free</span><span class=p>(</span><span class=n>ctx</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>ggml_init: found unused context 0
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>=== 2D Memory Layout ===
</span></span><span class=line><span class=cl>Shape: 4x3 (width x height)
</span></span><span class=line><span class=cl>Strides: [4, 16] bytes
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Logical view:
</span></span><span class=line><span class=cl>Row 0:      0      1      2      3
</span></span><span class=line><span class=cl>Row 1:      4      5      6      7
</span></span><span class=line><span class=cl>Row 2:      8      9     10     11
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Memory addresses:
</span></span><span class=line><span class=cl>Row 0:      0      4      8     12
</span></span><span class=line><span class=cl>Row 1:     16     20     24     28
</span></span><span class=line><span class=cl>Row 2:     32     36     40     44
</span></span><span class=line><span class=cl>ggml_free: context 0 with 1 objects has been freed. memory used = 208
</span></span></code></pre></div></details><br><h1 id=операции-върху-тензори>Операции върху тензори</h1><p>Разборът на целия изходен код би отнел много време, но тъй като той е доста прост - ще го оставя тук в сгъваем блок, ако ви интересува. Кодът поддържа SIMD чрез Arm Neon архитектура (на Mac-а) или AVX на х86</p><details><summary><b>Изходен код на операциите</b></summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_set_i8</span><span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>int8_t</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>int8_t</span> <span class=n>v</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>v</span><span class=p>;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_set_i16</span><span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>int16_t</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>int16_t</span> <span class=n>v</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>v</span><span class=p>;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_set_i32</span><span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>int32_t</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>int32_t</span> <span class=n>v</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>v</span><span class=p>;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_add_f32</span> <span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>z</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>y</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>z</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>];</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_acc_f32</span> <span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>x</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>];</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_acc1_f32</span><span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=n>v</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>v</span><span class=p>;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_sub_f32</span> <span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>z</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>y</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>z</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>];</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_set_f32</span> <span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=n>v</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>v</span><span class=p>;</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_cpy_f32</span> <span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>x</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>];</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_neg_f32</span> <span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>x</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=o>-</span><span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>];</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_mul_f32</span> <span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>z</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>y</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>z</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>*</span><span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>];</span> <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_div_f32</span> <span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>z</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=n>y</span><span class=p>)</span> <span class=p>{</span> <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=n>z</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>/</span><span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>];</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_mad_f32</span><span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=kr>restrict</span> <span class=n>y</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=kr>restrict</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=n>v</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>*</span><span class=n>v</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>inline</span> <span class=k>static</span> <span class=kt>void</span> <span class=nf>ggml_vec_dot_f32</span><span class=p>(</span><span class=k>const</span> <span class=kt>int</span> <span class=n>n</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span> <span class=kr>restrict</span> <span class=n>s</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=kr>restrict</span> <span class=n>x</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span> <span class=kr>restrict</span> <span class=n>y</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=n>ggml_float</span> <span class=n>sum</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>		<span class=n>sum</span> <span class=o>+=</span> <span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>*</span><span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>	<span class=p>}</span>
</span></span><span class=line><span class=cl>	<span class=o>*</span><span class=n>s</span> <span class=o>=</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// ... (SIMD оптимизации са пропуснати за кратност)
</span></span></span></code></pre></div></details><br><p>Този код в библиотеката фактически извършва всички операции, всички останали операции за работа с матрици и тензори ще използват кода за работа с вектори в крайна сметка.</p><p>Ето например, функцията за изчисляване на абсолютната стойност на тензор, която използва векторни операции за работа с данните.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// ggml_compute_forward_abs
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=nf>ggml_compute_forward_abs_f32</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_compute_params</span> <span class=o>*</span> <span class=n>params</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=k>const</span> <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>src0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>dst</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>assert</span><span class=p>(</span><span class=n>params</span><span class=o>-&gt;</span><span class=n>ith</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>assert</span><span class=p>(</span><span class=nf>ggml_are_same_shape</span><span class=p>(</span><span class=n>src0</span><span class=p>,</span> <span class=n>dst</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>params</span><span class=o>-&gt;</span><span class=n>type</span> <span class=o>==</span> <span class=n>GGML_TASK_INIT</span> <span class=o>||</span> <span class=n>params</span><span class=o>-&gt;</span><span class=n>type</span> <span class=o>==</span> <span class=n>GGML_TASK_FINALIZE</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>n</span>  <span class=o>=</span> <span class=nf>ggml_nrows</span><span class=p>(</span><span class=n>src0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>nc</span> <span class=o>=</span> <span class=n>src0</span><span class=o>-&gt;</span><span class=n>ne</span><span class=p>[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>assert</span><span class=p>(</span><span class=n>dst</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>  <span class=o>==</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=nf>assert</span><span class=p>(</span><span class=n>src0</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>==</span> <span class=k>sizeof</span><span class=p>(</span><span class=kt>float</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>ggml_vec_abs_f32</span><span class=p>(</span><span class=n>nc</span><span class=p>,</span> <span class=c1>// &lt;-- абсолютна стойност на вектора
</span></span></span><span class=line><span class=cl><span class=c1></span>                <span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=p>)</span> <span class=p>((</span><span class=kt>char</span> <span class=o>*</span><span class=p>)</span> <span class=n>dst</span><span class=o>-&gt;</span><span class=n>data</span>  <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=p>(</span> <span class=n>dst</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>])),</span>
</span></span><span class=line><span class=cl>                <span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=p>)</span> <span class=p>((</span><span class=kt>char</span> <span class=o>*</span><span class=p>)</span> <span class=n>src0</span><span class=o>-&gt;</span><span class=n>data</span> <span class=o>+</span> <span class=n>i</span><span class=o>*</span><span class=p>(</span><span class=n>src0</span><span class=o>-&gt;</span><span class=n>nb</span><span class=p>[</span><span class=mi>1</span><span class=p>])));</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h1 id=изчислителен-граф>Изчислителен граф</h1><p>GGML използва изчислителен граф за организация на операциите върху тензорите. Графът се състои от възли, където всеки възел представлява операция върху тензори. Възлите могат да бъдат свързани помежду си, образувайки верига от изчисления.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=c1>// computation graph
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>struct</span> <span class=n>ggml_cgraph</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>n_nodes</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>n_leafs</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>n_threads</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>size_t</span> <span class=n>work_size</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>work</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>nodes</span><span class=p>[</span><span class=n>GGML_MAX_NODES</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>grads</span><span class=p>[</span><span class=n>GGML_MAX_NODES</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span> <span class=n>leafs</span><span class=p>[</span><span class=n>GGML_MAX_NODES</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><p>Графът съдържа масиви от възли, градиенти и листа, както и размера на буфера за работа. Всеки възел в графа представлява тензор, който може да бъде резултат от операция или вход за други операции.</p><p>Също така заслужава внимание функцията за изчисляване на графа, която стартира нишки за изпълнение на операциите върху тензорите (ако е нужно разделяне на нишки). Тук е важно да се разбере, че възлите на графа са топологически подредени, и всеки възел може да зависи от резултатите на предишни възли. Затова, при изчисляване на графа, първо се изпълняват възлите, от които зависят други възли, а после се изпълняват възлите, които зависят от тях.</p><h1 id=автоматично-диференциране-и-обучение>Автоматично диференциране и обучение</h1><p>GGML поддържа автоматично диференциране и вградени оптимизатори, което позволява обучение на модели без ръчно изчисляване на градиенти.</p><h2 id=възможности-на-ggml-за-обучение>Възможности на GGML за обучение</h2><ul><li><strong>Автоматично диференциране</strong>: GGML автоматично изчислява градиенти</li><li><strong>Вградени оптимизатори</strong>: Adam, L-BFGS и други алгоритми за оптимизация</li><li><strong>Управление на паметта</strong>: Автоматично управление на графите от изчисления</li><li><strong>Обратно разпространение</strong>: Поддръжка на backward pass за градиенти</li></ul><h2 id=пример-за-обучение-на-линейна-регресия>Пример за обучение на линейна регресия</h2><p>Прост пример за обучение на линеен модел с помощта на вградения оптимизатор на GGML:</p><details><summary><b>Пример за обучение на линейна регресия</b></summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-c data-lang=c><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;ggml.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;stdio.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;stdlib.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;math.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;string.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&lt;time.h&gt;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=kt>bool</span> <span class=nf>is_close</span><span class=p>(</span><span class=kt>float</span> <span class=n>a</span><span class=p>,</span> <span class=kt>float</span> <span class=n>b</span><span class=p>,</span> <span class=kt>float</span> <span class=n>epsilon</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nf>fabs</span><span class=p>(</span><span class=n>a</span> <span class=o>-</span> <span class=n>b</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>epsilon</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>int</span> <span class=nf>main</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;=== GGML Simple Linear Regression Test ===</span><span class=se>\n\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_init_params</span> <span class=n>params</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=p>.</span><span class=n>mem_size</span> <span class=o>=</span> <span class=mi>128</span> <span class=o>*</span> <span class=mi>1024</span> <span class=o>*</span> <span class=mi>1024</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>.</span><span class=n>mem_buffer</span> <span class=o>=</span> <span class=nb>NULL</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>};</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_opt_params</span> <span class=n>opt_params</span> <span class=o>=</span> <span class=nf>ggml_opt_default_params</span><span class=p>(</span><span class=n>GGML_OPT_ADAM</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>opt_params</span><span class=p>.</span><span class=n>adam</span><span class=p>.</span><span class=n>alpha</span> <span class=o>=</span> <span class=mf>0.01f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;=== Simple Linear Regression Test ===</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Trying to fit: y = t0 + t1*x to polynomial data</span><span class=se>\n\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Create training data from polynomial y = x^2 + 2*x + 1
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kt>float</span> <span class=n>xi</span><span class=p>[]</span> <span class=o>=</span> <span class=p>{</span><span class=o>-</span><span class=mf>2.0f</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.0f</span><span class=p>,</span> <span class=mf>0.0f</span><span class=p>,</span> <span class=mf>1.0f</span><span class=p>,</span> <span class=mf>2.0f</span><span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>yi</span><span class=p>[</span><span class=mi>5</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=mi>5</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>yi</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=mf>2.0f</span> <span class=o>*</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=mf>1.0f</span><span class=p>;</span> <span class=c1>// y = x^2 + 2*x + 1
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int</span> <span class=n>n</span> <span class=o>=</span> <span class=k>sizeof</span><span class=p>(</span><span class=n>xi</span><span class=p>)</span> <span class=o>/</span> <span class=k>sizeof</span><span class=p>(</span><span class=n>xi</span><span class=p>[</span><span class=mi>0</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Training data:</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;x=%.2f -&gt; y=%.2f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>yi</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_context</span> <span class=o>*</span><span class=n>ctx0</span> <span class=o>=</span> <span class=nf>ggml_init</span><span class=p>(</span><span class=n>params</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span><span class=n>x</span> <span class=o>=</span> <span class=nf>ggml_new_tensor_1d</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=n>GGML_TYPE_F32</span><span class=p>,</span> <span class=n>n</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span><span class=n>y</span> <span class=o>=</span> <span class=nf>ggml_new_tensor_1d</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=n>GGML_TYPE_F32</span><span class=p>,</span> <span class=n>n</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=p>((</span><span class=kt>float</span> <span class=o>*</span><span class=p>)</span><span class=n>x</span><span class=o>-&gt;</span><span class=n>data</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>((</span><span class=kt>float</span> <span class=o>*</span><span class=p>)</span><span class=n>y</span><span class=o>-&gt;</span><span class=n>data</span><span class=p>)[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>yi</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span><span class=n>t0</span> <span class=o>=</span> <span class=nf>ggml_new_f32</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=mf>0.0f</span><span class=p>);</span> <span class=c1>// bias
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span><span class=n>t1</span> <span class=o>=</span> <span class=nf>ggml_new_f32</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=mf>0.1f</span><span class=p>);</span> <span class=c1>// weight
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>    <span class=c1>// initialize auto-diff parameters:
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>ggml_set_param</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=n>t0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>ggml_set_param</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=n>t1</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Initial parameters: t0(bias)=%.4f, t1(weight)=%.4f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>t0</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span> <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>t1</span><span class=p>,</span> <span class=mi>0</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// f = sum_i[(t0 + t1*x_i - y_i)^2]/(2n)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>struct</span> <span class=n>ggml_tensor</span> <span class=o>*</span><span class=n>f</span> <span class=o>=</span>
</span></span><span class=line><span class=cl>        <span class=nf>ggml_div</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=nf>ggml_sum</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=nf>ggml_sqr</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                   <span class=nf>ggml_sub</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                            <span class=nf>ggml_add</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                     <span class=nf>ggml_mul</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=nf>ggml_repeat</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=n>t1</span><span class=p>,</span> <span class=n>x</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>                                                     <span class=nf>ggml_repeat</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=n>t0</span><span class=p>,</span> <span class=n>x</span><span class=p>)),</span>
</span></span><span class=line><span class=cl>                                            <span class=n>y</span><span class=p>))),</span>
</span></span><span class=line><span class=cl>                 <span class=nf>ggml_new_f32</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=mf>2.0f</span> <span class=o>*</span> <span class=n>n</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Check initial predictions and loss
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>struct</span> <span class=n>ggml_cgraph</span> <span class=n>gf_init</span> <span class=o>=</span> <span class=nf>ggml_build_forward</span><span class=p>(</span><span class=n>f</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>ggml_graph_compute</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>gf_init</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Initial loss: %.6f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=mi>0</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Compute initial predictions manually to show them
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>Initial predictions:</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>pred</span> <span class=o>=</span> <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>t0</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>t1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;x=%.2f: pred=%.3f, true=%.2f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>pred</span><span class=p>,</span> <span class=n>yi</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>Training...</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>enum</span> <span class=n>ggml_opt_result</span> <span class=n>res</span> <span class=o>=</span> <span class=nf>ggml_opt</span><span class=p>(</span><span class=nb>NULL</span><span class=p>,</span> <span class=n>opt_params</span><span class=p>,</span> <span class=n>f</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Training result: %s</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>res</span> <span class=o>==</span> <span class=n>GGML_OPT_OK</span> <span class=o>?</span> <span class=s>&#34;SUCCESS&#34;</span> <span class=o>:</span> <span class=n>res</span> <span class=o>==</span> <span class=n>GGML_OPT_DID_NOT_CONVERGE</span> <span class=o>?</span> <span class=s>&#34;DID_NOT_CONVERGE&#34;</span>
</span></span><span class=line><span class=cl>                                                                             <span class=o>:</span> <span class=s>&#34;OTHER&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>Final parameters: t0(bias)=%.4f, t1(weight)=%.4f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>t0</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span> <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>t1</span><span class=p>,</span> <span class=mi>0</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Check final loss
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>struct</span> <span class=n>ggml_cgraph</span> <span class=n>gf_final</span> <span class=o>=</span> <span class=nf>ggml_build_forward</span><span class=p>(</span><span class=n>f</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>ggml_graph_compute</span><span class=p>(</span><span class=n>ctx0</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>gf_final</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Final loss: %.6f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=mi>0</span><span class=p>));</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Show final predictions
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;</span><span class=se>\n</span><span class=s>Final predictions:</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>total_error</span> <span class=o>=</span> <span class=mf>0.0f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>pred</span> <span class=o>=</span> <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>t0</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=nf>ggml_get_f32_1d</span><span class=p>(</span><span class=n>t1</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>error</span> <span class=o>=</span> <span class=nf>fabsf</span><span class=p>(</span><span class=n>pred</span> <span class=o>-</span> <span class=n>yi</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>        <span class=n>total_error</span> <span class=o>+=</span> <span class=n>error</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;x=%.2f: pred=%.3f, true=%.2f, error=%.3f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>xi</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>pred</span><span class=p>,</span> <span class=n>yi</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>error</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=nf>printf</span><span class=p>(</span><span class=s>&#34;Average error: %.4f</span><span class=se>\n</span><span class=s>&#34;</span><span class=p>,</span> <span class=n>total_error</span> <span class=o>/</span> <span class=n>n</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nf>ggml_free</span><span class=p>(</span><span class=n>ctx0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></details><br><p><strong>Особености на примера:</strong></p><ul><li><strong>Простота</strong>: Линейната регресия е една от най-простите и надеждни задачи в ML</li><li><strong>Гарантирана сходимост</strong>: Изпъкнала задача за оптимизация винаги се сходи</li><li><strong>Разбираем резултат</strong>: Лесно за проверка на правилността на обучението</li><li><strong>Минимални изисквания</strong>: Използва само основни операции на GGML</li></ul><p><strong>Предимства на вградения оптимизатор на GGML:</strong></p><ul><li>Автоматично управление на паметта на графите</li><li>Оптимизирани алгоритми (Adam, L-BFGS)</li><li>Отсъствие на проблеми с препълване на стека</li><li>По-малко код и по-лесен за използване</li></ul></div></div><div class=footer><div class=footer-social><span class="social-icon social-icon-twitter"><a href=https://twitter.com/zerostaticio title=twitter target=_blank rel=noopener><img src=/images/social/twitter.svg width=24 height=24 alt=twitter>
</a></span><span class="social-icon social-icon-github"><a href=https://github.com/zerostaticthemes/hugo-winston-theme title=github target=_blank rel=noopener><img src=/images/social/github.svg width=24 height=24 alt=github>
</a></span><span class="social-icon social-icon-linkedin"><a href=https://www.linkedin.com title=linkedin target=_blank rel=noopener><img src=/images/social/linkedin.svg width=24 height=24 alt=linkedin></a></span></div></div></div><script type=text/javascript src=/js/bundle.min.5993fcb11c07dea925a3fbd58c03c7f1857197c35fccce3aa963a12c0b3c9960.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]],packages:{"[+]":["unicode"]}},loader:{load:["ui/safe","[tex]/unicode"]},svg:{scale:1,minScale:.5,mtextInheritFont:!0,merrorInheritFont:!0,mathmlSpacing:!1,skipAttributes:{},exFactor:.5,displayAlign:"center",displayIndent:"0",fontCache:"local"},options:{enableMenu:!0,renderActions:{addMenu:[0,"",""]}},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(()=>{document.querySelectorAll("mjx-container").forEach(e=>{e.hasAttribute("display")?e.classList.add("math-display"):e.classList.add("math-inline")})})}}</script><style>.MathJax{margin:0 .15em}.MathJax[jax=SVG] text{font-family:inherit}</style></body></html>