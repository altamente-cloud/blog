<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><title>Вероятност, условна вероятност и теорема на Бейс - learn.altamente.cloud</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=/favicon.png><link rel=canonical href=/posts/bayes_bg/><link rel=stylesheet href=/css/style.min.9ace1889e342b746efbfbd99b2bcd2af4d96d96fa6c8dac480411cf33a84c4fd.css><link rel=stylesheet href=/assets/css/extended.min.75f2a96901bf25eddcf37edc9a30bebc25c59bb8d45432f08aef5c56e7a38d1f.css><meta name=description content="Интуитивно обяснение на вероятността, условната вероятност и теоремата на Бейс за начинаещи"><meta property="og:title" content="Вероятност, условна вероятност и теорема на Бейс"><meta property="og:type" content="website"><meta property="og:url" content="/posts/bayes_bg/"><meta property="og:image" content="/images/bayes/head.jpg"><meta property="og:description" content="Интуитивно обяснение на вероятността, условната вероятност и теоремата на Бейс за начинаещи"><meta name=twitter:card content="summary"><meta name=twitter:site content="@zerostaticio"><meta name=twitter:creator content="@zerostaticio"><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&amp;display=swap" rel=stylesheet></head><body class='page frame page-blog-single'><div id=menu-main-mobile class=menu-main-mobile><ul class=menu><li class=menu-item-начало><a href=/>Начало</a></li><li class=menu-item-статии><a href=/posts/>Статии</a></li><li class="menu-item-за сайта"><a href=/about/>За сайта</a></li><li class=menu-item-по теми><a href=/tags/>По теми</a></li></ul></div><div id=wrapper class=wrapper><div class=header><a class=header-logo href=/>learn.altamente.cloud</a><div class=menu-main><ul><li class=menu-item-начало><a href=/><span>Начало</span></a></li><li class=menu-item-статии><a href=/posts/><span>Статии</span></a></li><li class="menu-item-за сайта"><a href=/about/><span>За сайта</span></a></li><li class=menu-item-по теми><a href=/tags/><span>По теми</span></a></li></ul></div><div id=toggle-menu-main-mobile class=hamburger-trigger><button class=hamburger>Menu</button></div></div><div class=blog><div class=intro><h1>Вероятност, условна вероятност и теорема на Бейс<span class=dot>.</span></h1><img alt="Вероятност, условна вероятност и теорема на Бейс" src=/images/bayes/head.jpg><p class=image-description>Теорема на Бейс в офиса на Autonomy в Кеймбридж (взето от Wikipedia)</p></div><div class=tags><a href=/%20/tags/bayes class=tag>bayes</a>
<a href=/%20/tags/probability class=tag>probability</a>
<a href=/%20/tags/prior class=tag>prior</a>
<a href=/%20/tags/posterior class=tag>posterior</a>
<a href=/%20/tags/likelihood class=tag>likelihood</a>
<a href=/%20/tags/math class=tag>math</a></div><div class=content><h2 id=няколко-думи-за-вероятността>Няколко думи за вероятността</h2><p>Теорията на вероятностите е в основата на машинното обучение. Тя предоставя математически апарат за работа с несигурност и шум в данните — нещо неизбежно в реалния свят. Вероятностите изразяват степента на увереност в предсказанията, а много от моделите — като логистичната регресия, наивния баесов класификатор и дори невронните мрежи — се обучават чрез оптимизиране на функции, свързани с вероятности, като логаритъм на правдоподобието или крос-ентропия.</p><p>Байесовите методи позволяват включване на предварителни знания чрез априорни разпределения, а вероятностните оценки помагат при сравняване и избиране на модели. С други думи, теорията на вероятностите е езикът, чрез който формулираме, обучаваме и интерпретираме машинни модели.</p><p>В тази статия ще разгледаме основите на теорията на вероятностите, условната вероятност и теоремата на Бейс. Ще се опитам да обясня всичко по възможно най-простия начин, без сложни формули и термини (освен ако не е абсолютно необходимо). Статията е предназначена за начинаещи.</p><h2 id=вероятностно-пространство-събития-и-вероятности>Вероятностно пространство, събития и вероятности</h2><h3 id=множество>Множество</h3><p>Нека започнем с определението на множество. Множеството е колекция от обекти, които могат да бъдат всичко: числа, хора, автомобили и т.н.</p><p>Множеството може да бъде крайно или безкрайно. Например, множеството на всички естествени числа (от 1 до безкрайност) е безкрайно, а множеството на всички хора на Земята е крайно.</p><p>Математически множеството се обозначава с фигурни скоби. Например, множеството на всички естествени числа от 1 до 10:</p>$$
\{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}
$$<p>Множеството може да бъде празно: \(\emptyset\) или \(\{\}\). Празното множество не съдържа нито един елемент.</p><p><strong>Пример:</strong></p><p>Множество на всички естествени числа:</p>$$
\mathbb{N} = \{1, 2, 3, 4, 5, \ldots\}
$$<p>Множество на всички цели числа:</p>$$
\mathbb{Z} = \{\ldots, -3, -2, -1, 0, 1, 2, 3, \ldots\}
$$<p>Множество на всички реални числа:</p>$$
\mathbb{R} = \{x \mid x \text{ е реално число}\}
$$<h3 id=подмножество>Подмножество</h3><p>Ако множеството B е част от множеството A, то B е подмножество на A.</p><p>Например, ако \(A = \{1,2,3\}\), то възможните подмножества са: \(\emptyset\), \(\{1\}\), \(\{2\}\), \(\{3\}\), \(\{1,2\}\), \(\{1,3\}\), \(\{2,3\}\), \(\{1,2,3\}\).</p><p>Обозначава се като \(B \subseteq A\). Ако B не е подмножество на A, пишем \(B \nsubseteq A\).</p><h3 id=вероятностно-пространство-sample-space>Вероятностно пространство (Sample Space)</h3><p>Вероятностното пространство е множеството на всички възможни изходи от експеримента.</p><p>Например, при хвърляне на зар вероятностното пространство е:</p>$$
S = \{1, 2, 3, 4, 5, 6\}
$$<p>При хвърляне на монета:</p>$$
S = \{\text{ези}, \text{тура}\}
$$<p>Ако хвърляме зара два пъти:</p>$$
S = \{ (1,1), (1,2), \ldots, (6,5), (6,6) \}
$$<blockquote><p>Вероятностното пространство винаги зависи от конкретния експеримент, който провеждаме.</p></blockquote><p>Пример за вероятностно пространство при хвърляне на зар:</p><img src=/images/bayes/samplespace.png alt="Вероятностно пространство" width=300><h3 id=събитие>Събитие</h3><p>Всяко подмножество на вероятностното пространство се нарича събитие (ще го означим като E).</p><p>Например, при хвърляне на зар събитието &ldquo;падна се четно число&rdquo;:</p>$$
E = \{2, 4, 6\}
$$<img src=/images/bayes/event.png alt=Събитие width=300><blockquote><p>Да запомним: събитието \(E\) е подмножество на вероятностното пространство \(S\), т.е. \(E \subseteq S\).</p></blockquote><p>E може да бъде празно (например, събитието &ldquo;падна се число по-голямо от 6&rdquo; при хвърляне на зар).</p><h3 id=вероятност>Вероятност</h3><p>Вероятността е мярка за това колко е вероятно дадено събитие да се случи. Вероятността на събитието E се обозначава с \(P(E)\).</p><p>Интуитивно, вероятността на събитието E е отношението на броя на благоприятните изходи към общия брой изходи в вероятностното пространство (т.е. броя на елементите в E към броя на елементите в S):</p>$$
P(E) = \frac{|E|}{|S|} \tag{1}
$$<p>където \(|E|\) е броят на елементите в E.</p><p><strong>Пример:</strong></p><p>За примера със зара вероятността да се падне четно число:</p>$$
P(E) = \frac{3}{6} = \frac{1}{2}
$$<p>Тъй като \(|E| \leq |S|\), вероятността на събитието E винаги е в интервала от 0 до 1.</p><ul><li>При вероятност 0 събитието никога няма да се случи.</li><li>При вероятност 1 събитието винаги ще се случи.</li></ul><blockquote><p><em>Важно</em>: Да изчисляваме вероятността по такъв начин, чрез броене на благоприятните изходи, можем само ако всички изходи са <em>равновероятни</em>. Този подход се нарича <strong>класически</strong>.</p></blockquote><p>Но ако, например, нашият зар е неравновероятен (с изместен център на тежестта), то вероятността да се падне 1, например, вече няма да бъде 1/6.</p><p>Ако поне един от изходите е по-вероятен, такъв метод престава да отразява реалната вероятност и трябва да използваме действителните честоти. Така, ако хвърляме зар 100 пъти и 20 пъти се пада 1, то вероятността да се падне 1 ще бъде 20/100 = 0.2.</p><p>В такава ситуация вероятността на ВСЯКО ОТДЕЛНО събитие може да се изчисли чрез <strong>честотния</strong> подход:</p>$$
P(x) = \frac{n(x)}{N}
$$<p>където \(n(x)\) е броят пъти, когато се е случило събитието x, а \(N\) е общият брой изпитания.</p><p>За нашия пример със зара и събитието &ldquo;падна се четно число&rdquo;:</p>$$
P(E) = \frac{n(E)}{N} = \frac{3}{6} = \frac{1}{2}
$$<p>където \(n(E)\) е броят пъти, когато се е паднало четно число, а \(N\) е общият брой хвърляния.</p><blockquote><p>В тази статия ще използваме класическия подход, тъй като той е по-прост и интуитивно разбираем. Но крайните формули ще бъдат еднакви за двата подхода.</p></blockquote><h3 id=две-събития-обединение-и-сечение>Две събития: обединение и сечение</h3><p>Нека имаме две събития E и F. Тяхното сечение (\(E \cap F\)) е събитието, когато се случват и двете събития E и F. А обединението (\(E \cup F\)) е събитието, когато се случва поне едно от двете събития E или F.</p><p>Например, ако E е &ldquo;падна се четно число&rdquo;, а F е &ldquo;падна се число по-голямо от 3&rdquo;, то:</p>$$
E = \{2, 4, 6\}
$$$$
F = \{4, 5, 6\}
$$$$
E \cup F = \{2, 4, 5, 6\}
$$$$
E \cap F = \{4, 6\}
$$<p>Графично това е сечението на два кръга на диаграмата на Вен:</p><img src=/images/bayes/intersect.png alt="Сечение и обединение на две събития" width=300><p>Вероятността, че ще се случи поне едно от двете събития E или F, се обозначава с \(P(E \cup F)\) и се изчислява по формулата:</p>$$
P(E \cup F) = \frac{|E|}{|S|} + \frac{|F|}{|S|} - \frac{|E \cap F|}{|S|} = \frac{|E \cup F|}{|S|} = P(E) + P(F) - P(E \cap F) \tag{2}
$$<p>Тук \(P(E \cup F)\) е вероятността, че ще се случи поне едно от двете събития.</p><blockquote><p>Формулата отчита, че ако събитията E и F се пресичат, то елементите на сечението ще бъдат преброени два пъти, затова изваждаме \(P(E \cap F)\).</p></blockquote><p>Сега да разгледаме вероятността на сечението на две събития:</p><p>В нашия пример E е &ldquo;падна се четно число&rdquo;, а F е &ldquo;падна се число по-голямо от 3&rdquo;. Тогава:</p>$$
P(E \cap F) = \frac{|E \cap F|}{|S|} = \frac{|\{4, 6\}|}{|S|} = \frac{2}{6} = \frac{1}{3}
$$<h3 id=зависимост-и-независимост-на-събитията>Зависимост и независимост на събитията</h3><p>Сега да разгледаме зависимостта и независимостта на събитията. Ако събитията E и F са независими, то вероятността на тяхното сечение е равна на произведението на вероятностите на всяко от тях:</p>$$
P(E \cap F) = \frac{|E \cap F|}{|S|} = \frac{|E|}{|S|} \cdot \frac{|F|}{|S|} = P(E) \cdot P(F) \tag{3}
$$<p>За да разберем тази формула интуитивно, нека си представим следното:</p><p>Ако \(P(E) = |E|/|S|\) е делът на благоприятните изходи E в пространството S, а \(P(F) = |F|/|S|\) е делът на благоприятните изходи F в пространството S, то ако събитията са <strong>независими</strong>, то шансът, че в частта на благоприятните изходи E ще има и част от благоприятните изходи F, ще бъде същият като шанса, че във всички изходи S ще има част от благоприятните изходи F.</p><p>А ако е така, то тази част от S, която ще бъде в E (\(P(E) \cdot |S|\)), има шанс \(P(F)\) да съдържа благоприятни изходи F. Тоест \(P(E) \cdot P(F) = P(E \cap F)\).</p><p>Ако вероятността на събитието E не зависи от това дали е настъпило събитието F, то събитията E и F се наричат независими.</p><p>Например, ако хвърляме зар два пъти, то:</p>$$
S = \{ (1,1), (1,2), \ldots, (6,6) \}
$$<p>Нека E е &ldquo;първото хвърляне е четно&rdquo;, а F е &ldquo;второто хвърляне е четно&rdquo;. Тези събития са независими, тъй като вероятността за четно първо хвърляне не зависи от резултата от второто хвърляне.</p><p>За всяко хвърляне половината от изходите са четни:</p>$$
P(E) = P(F) = \frac{3}{6} = \frac{1}{2}
$$<p>Вероятността и двете събития да се случат:</p>$$
P(E \cap F) = P(E) \cdot P(F)
$$<p>В нашия пример след първото от всички възможни изходи благоприятни са точно половината, а след второто хвърляне от тази половина също половината. Тоест:</p>$$
P(E \cap F) = P(E) \cdot P(F) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}
$$<blockquote><p>Тази формула е валидна само за независими събития.</p></blockquote><p>Ако вероятността на събитието E зависи от това дали е настъпило събитието F, то събитията E и F се наричат зависими. Обозначава се като \(P(E|F)\) — вероятността на събитието E при условие, че е настъпило събитието F.</p><blockquote><p>Да запомним, че ако събитията E и F са <strong>независими</strong>, то:</p>$$P(E|F) = P(E) \tag{4}$$<p>Ако събитията са зависими, то:</p>$$P(E|F) \neq P(E)$$</blockquote><p>За по-добро разбиране нека разгледаме два примера:</p><p><strong>Пример 1:</strong></p><p>Нека F е &ldquo;падна се число по-голямо от 3&rdquo;, а E е &ldquo;падна се четно число&rdquo;. Тогава:</p>$$
P(F) = \frac{3}{6} = \frac{1}{2}
$$$$
P(E) = \frac{3}{6} = \frac{1}{2}
$$<p>Но</p>$$
P(E|F) = \frac{|E \cap F|}{|F|} = \frac{|\{4, 6\}|}{|\{4, 5, 6\}|} = \frac{2}{3}
$$<p>което не е равно на \(P(E)\).</p><img src=/images/bayes/conditional.png alt="Зависимост на събитията" width=300><p>(От 3 четни числа {2, 4, 6} две са по-големи от 3 — {4, 6})</p><p>Ако събитията бяха независими, то \(P(E|F) = P(E)\). В случая \(P(E|F) > P(E)\), следователно събитията са зависими.</p><p><strong>Пример 2:</strong></p><p>Нека E е &ldquo;падна се едно от числата {1,2,3}&rdquo;, а F е &ldquo;падна се едно от числата {1,2,4,6}&rdquo;.</p><p>Ако е настъпило събитието F, то за да настъпи събитието E (падна се едно от {1,2,3}), трябва да се падне 1 или 2. Да проверим това математически:</p>$$
P(F) = \frac{4}{6} = \frac{2}{3}
$$$$
P(E) = \frac{3}{6} = \frac{1}{2}
$$<p>Сега ще намерим \(P(E|F)\):</p>$$
P(E|F) = \frac{|E \cap F|}{|F|} = \frac{2}{4} = \frac{1}{2}
$$<p>Обърнете внимание: тъй като знаем, че е настъпило събитието F (паднало се е едно от {1,2,4,6}), разглеждаме само тези числа. В числителя е \(E \cap F\), в знаменателя – броят на елементите в F.</p><p>Да сравним \(P(E|F)\) и \(P(E)\): те са равни, следователно събитията са независими.</p><p>Ако ни съобщят: &ldquo;Падна се число от F (тоест 1, 2, 4 или 6)&rdquo;, това не променя вероятността за попадане в E (без да знаем F: {1,2,3} от {1,2,3,4,5,6}; знаейки F: {1,2} от {1,2,4,6}). Тя остава 3/6 = 2/4 = 0.5. Следователно, информацията за F не помага да разберем дали се е случило E.</p><blockquote><p><strong>Независимостта е информационна слепота:</strong><br>Знанието за едно събитие не дава информация за другото.</p></blockquote><p>Какво да правим, ако събитията са зависими? На какво е равна вероятността на събитието E при условие, че е настъпило събитието F?</p><p>Аналогично на начина, по който обяснихме формулата за сечението на вероятността на независими събития, можем да разсъждаваме и за зависими събития.</p><p>Ако събитието F е настъпило, можем да разглеждаме само онези изходи, които съответстват на събитието F. В случай на зависими събития, делът на благоприятните изходи Е, които се пресичат с F, вече не е равен на дела на всички благоприятни изходи E в пространството S.</p><p>А ако е така, когато е настъпило F - шансът да се случи и E се е променил! Можем да запишем формула за условната вероятност на събитието E при условие, че е настъпило събитието F:</p>$$
P(E \cap F) = P(E|F) \cdot P(F) \tag{5}
$$<p>От това следва, че:</p>$$
P(E|F) = \frac{P(E \cap F)}{P(F)} \tag{6}
$$<p>Тъй като сечението на две събития E и F е събитието, което е настъпило едновременно, можем аналогично да изразим вероятността P(E∩F) чрез условната вероятност на събитието F при условие, че е настъпило събитието E:</p>$$
P(E \cap F) = P(F|E) \cdot P(E) \tag{7}
$$<p>Като приравним десните страни на (5) и (7), получаваме:</p>$$
P(E|F) \cdot P(F) = P(F|E) \cdot P(E)
$$<p>или</p>$$
P(E|F) = \frac{P(F|E) \cdot P(E)}{P(F)} \tag{8}
$$<blockquote><p><strong>ТОВА Е ТЕОРЕМАТА НА БЕЙС!</strong></p></blockquote><p>Това е формула, която свързва условните вероятности на две събития E и F. Тя позволява да изчислим вероятността на събитието E при условие, че е настъпило събитието F, знаейки вероятността на събитието F при условие, че е настъпило събитието E.</p><p>Често P(F) е неизвестна, но може да бъде изразена чрез пълната вероятност на събитието F:</p><p><strong>Формула на пълната вероятност</strong>:</p>$$
P(F) = P(F|E) \cdot P(E) + P(F| \neg E) \cdot P(\neg E) \tag{9}
$$<p>където \(\neg E\) е събитието, противоположно на събитието E.</p><blockquote><p><strong>Теоремата на Бейс</strong> е формула, която свързва условните вероятности на две събития E и F. Тя позволява да изчислим вероятността на събитието E при условие, че е настъпило събитието F, знаейки вероятността на събитието F при условие, че е настъпило събитието E.</p></blockquote><p>Това е много полезна формула, която позволява да изчисляваме вероятности на събития, когато са известни други вероятности. Тя се използва широко в статистиката, машинното обучение и други области.</p><p>Всеки от тези елементи на формулата има своето значение:</p><ul><li><strong>P(E|F)</strong> — апостериорна вероятност (това, което искаме да узнаем)</li><li><strong>P(E)</strong> — априорна вероятност (първоначално предположение)</li><li><strong>P(F|E)</strong> — правдоподобие (likelihood)</li><li><strong>P(F)</strong> — пълна вероятност на събитието F</li></ul><p>Детайлното разглеждане на всеки от тези елементи излиза извън обхвата на тази статия, но като цяло можем да кажем, че:</p><blockquote><ul><li><strong>A priori</strong> (априорна) вероятност — това е вероятността на събитието преди получаването на нова информация.</li><li><strong>A posteriori</strong> (апостериорна) вероятност — това е вероятността на събитието след получаването на нова информация.</li><li><strong>Правдоподобие</strong> (likelihood) — това е вероятността да наблюдаваме данните, ако хипотезата е вярна.</li><li><strong>Пълна вероятност</strong> (marginal likelihood) — това е вероятността да наблюдаваме данните, независимо от хипотезата.</li></ul></blockquote><p>Всяка от тези вероятности има своето значение и се използва като основа за алгоритми на статистиката и машинното обучение, като байесови мрежи, наивен байесов класификатор, MLE, ELBO, MCMC и много други.</p><h2 id=дядо-иван-условна-вероятност-и-теоремата-на-бейс>Дядо Иван, условна вероятност и теоремата на Бейс</h2><h3 id=няколко-примера>Няколко примера</h3><p>Да разгледаме пример.<br>Дядо Иван попадна на бирен фестивал. Организаторите устроиха томбола: от 50 бутилки (40 светли, от които 9 безалкохолни, и 10 тъмни, от които 3 безалкохолни).</p><img src=/images/bayes/beer.png alt="Условна вероятност" width=500><p><strong>Пример 1:</strong></p><p>Нека започнем с прост въпрос: &ldquo;Каква е вероятността бутилката да е безалкохолна?&rdquo; Отговорът е прост: 12/50 = 0.24.<br>Но какво ако знаем, че бутилката е тъмна? Каква е вероятността тя да е безалкохолна?<br>В този случай разглеждаме само тъмните бутилки. От 10 тъмни бутилки 3 са безалкохолни, следователно вероятността е 3/10 = 0.3.</p><p>Сега с помощта на формулата на Бейс можем да изразим вероятността бутилката да е тъмна, при условие че е безалкохолна:</p>$$
P(\text{тъмна} | \text{безалкохолна}) = \frac{P(\text{безалкохолна} | \text{тъмна}) \cdot P(\text{тъмна})}{P(\text{безалкохолна})} = \frac{3/10 \cdot 10/50}{12/50} = \frac{3}{12} = 0.25
$$<p>Това съвпада с интуитивното разбиране: от 12 безалкохолни бутилки 3 са тъмни, следователно вероятността безалкохолната бутилка да е тъмна е 0.25.</p><p><strong>Пример 2:</strong></p><p>Дядо Иван и приятелят му Петър се състезават кой по-добре определя бирата.</p><ol><li>И двамата независимо опитват една и съща бутилка. Дядо Иван правилно определя типа бира с вероятност 0.7, а Петър – с вероятност 0.65. Каква е вероятността поне един от тях да определи правилно типа на случайно избраната бира?</li></ol><p>Решение:</p>$$
\begin{aligned}
P(\text{поне един}) &= 1 - P(\text{и двамата сгрешиха}) \\
&= 1 - P(\text{грешка дядо Иван}) \cdot P(\text{грешка Петър}) \\
&= 1 - (1 - 0.7) \cdot (1 - 0.65) \\
&= 1 - 0.3 \cdot 0.35 \\
&= 1 - 0.105 = 0.895
\end{aligned}
$$<ol start=2><li>Дядо Иван със завързани очи опита бирата и каза, че е тъмна алкохолна. Известно е, че той правилно определя светлата алкохолна с вероятност 0.7, светлата безалкохолна с вероятност 0.6, тъмната алкохолна с вероятност 0.8 и тъмната безалкохолна с вероятност 0.7. Ако дядо Иван заяви, че бирата е светла, каква е вероятността тя да е алкохолна?</li></ol><p><strong>Решение:</strong></p><p>Брой бутилки от различни типове:</p><ul><li>Светла алкохолна: 40 - 9 = 31 бутилки</li><li>Светла безалкохолна: 9 бутилки</li><li>Тъмна алкохолна: 10 - 3 = 7 бутилки</li><li>Тъмна безалкохолна: 3 бутилки</li></ul><p>Вероятности за различните типове бира:</p>$$P(\text{светла алкохолна}) = \frac{31}{50} = 0.62$$<p><br></p>$$P(\text{светла безалкохолна}) = \frac{9}{50} = 0.18$$<p><br></p>$$P(\text{тъмна алкохолна}) = \frac{7}{50} = 0.14$$<p><br></p>$$P(\text{тъмна безалкохолна}) = \frac{3}{50} = 0.06$$<p>Да определим вероятностите за определяне на &ldquo;светла&rdquo; за всеки тип бира:</p><ul><li>Ако бирата е светла алкохолна: $$P(\text{ще каже "светла"} | \text{светла алкохолна}) = 0.7$$</li><li>Ако бирата е светла безалкохолна: $$P(\text{ще каже "светла"} | \text{светла безалкохолна}) = 0.6$$</li><li>Ако бирата е тъмна алкохолна: $$P(\text{ще каже "светла"} | \text{тъмна алкохолна}) = 1 - 0.8 = 0.2$$</li><li>Ако бирата е тъмна безалкохолна: $$P(\text{ще каже "светла"} | \text{тъмна безалкохолна}) = 1 - 0.7 = 0.3$$</li></ul><p>По формулата на пълната вероятност:</p>$$
\begin{aligned}
P(\text{ще каже "светла"}) &= \sum_i P(\text{ще каже "светла"} | \text{тип}_i) \cdot P(\text{тип}_i) \\
&= 0.7 \cdot 0.62 + 0.6 \cdot 0.18 + 0.2 \cdot 0.14 + 0.3 \cdot 0.06 \\
&= 0.434 + 0.108 + 0.028 + 0.018 = 0.588
\end{aligned}
$$<p>Нека намерим вероятността за алкохолна бира:<br></p>$$P(\text{алкохолна}) = P(\text{светла алкохолна}) + P(\text{тъмна алкохолна}) = 0.62 + 0.14 = 0.76$$<p>Сега използваме формулата на Бейс (8). Първо ще намерим вероятността дядо Иван да каже &ldquo;светла&rdquo; при условие, че бирата е алкохолна:</p>$$
\begin{aligned}
P(\text{ще каже "светла"} | \text{алкохолна}) &= \frac{P(\text{ще каже "светла"} | \text{светла алк}) \cdot P(\text{светла алк})}{P(\text{алкохолна})} \\
&+ \frac{P(\text{ще каже "светла"} | \text{тъмна алк}) \cdot P(\text{тъмна алк})}{P(\text{алкохолна})} \\
&= \frac{0.7 \cdot 0.62 + 0.2 \cdot 0.14}{0.76} \\
&= \frac{0.434 + 0.028}{0.76} \\
&= \frac{0.462}{0.76} = 0.6079
\end{aligned}
$$<p>Накрая прилагаме формулата на Бейс:</p>$$
\begin{aligned}
P(\text{алкохолна} | \text{ще каже "светла"}) &= \frac{P(\text{ще каже "светла"} | \text{алкохолна}) \cdot P(\text{алкохолна})}{P(\text{ще каже "светла"})} \\
&= \frac{0.6079 \cdot 0.76}{0.588} \\
&= \frac{0.462}{0.588} \approx 0.7857
\end{aligned}
$$<p>Следователно вероятността бирата да е алкохолна, при условие че дядо Иван е казал &ldquo;светла&rdquo;, е 0.7857.</p><h2 id=заключение>Заключение</h2><p>В тази статия разгледахме основите на вероятността, условната вероятност и теоремата на Бейс. Те са важни концепции в статистиката и машинното обучение, които ни помагат да разберем как да работим с вероятности и как да правим изводи на базата на наличната информация.</p></div></div><div class=footer><div class=footer-social><span class="social-icon social-icon-twitter"><a href=https://twitter.com/zerostaticio title=twitter target=_blank rel=noopener><img src=/images/social/twitter.svg width=24 height=24 alt=twitter>
</a></span><span class="social-icon social-icon-github"><a href=https://github.com/zerostaticthemes/hugo-winston-theme title=github target=_blank rel=noopener><img src=/images/social/github.svg width=24 height=24 alt=github>
</a></span><span class="social-icon social-icon-linkedin"><a href=https://www.linkedin.com title=linkedin target=_blank rel=noopener><img src=/images/social/linkedin.svg width=24 height=24 alt=linkedin></a></span></div></div></div><script type=text/javascript src=/js/bundle.min.5993fcb11c07dea925a3fbd58c03c7f1857197c35fccce3aa963a12c0b3c9960.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]],packages:{"[+]":["unicode"]}},loader:{load:["ui/safe","[tex]/unicode"]},svg:{scale:1,minScale:.5,mtextInheritFont:!0,merrorInheritFont:!0,mathmlSpacing:!1,skipAttributes:{},exFactor:.5,displayAlign:"center",displayIndent:"0",fontCache:"local"},options:{enableMenu:!0,renderActions:{addMenu:[0,"",""]}},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(()=>{document.querySelectorAll("mjx-container").forEach(e=>{e.hasAttribute("display")?e.classList.add("math-display"):e.classList.add("math-inline")})})}}</script><style>.MathJax{margin:0 .15em}.MathJax[jax=SVG] text{font-family:inherit}</style></body></html>